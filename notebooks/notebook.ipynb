{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "15029f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- STAGE 1 START ---\n",
      "Using data_path: '../data/f1_world_champion/'\n",
      "Successfully loaded: results.csv (Shape: (26759, 18))\n",
      "Successfully loaded: races.csv (Shape: (1125, 18))\n",
      "Successfully loaded: drivers.csv (Shape: (861, 9))\n",
      "Successfully loaded: constructors.csv (Shape: (212, 5))\n",
      "Successfully loaded: qualifying.csv (Shape: (10494, 9))\n",
      "Successfully loaded: circuits.csv (Shape: (77, 9))\n",
      "Successfully loaded: status.csv (Shape: (139, 2))\n",
      "Successfully loaded: constructor_results.csv (Shape: (12625, 5))\n",
      "Successfully loaded: lap_times.csv (Shape: (589081, 6))\n",
      "Successfully loaded: pit_stops.csv (Shape: (11371, 7))\n",
      "Successfully loaded: sprint_results.csv (Shape: (360, 16))\n",
      "Defining relevant status IDs for analysis...\n",
      "Defined 130 relevant status IDs.\n",
      "Shape before filtering results: (26759, 18)\n",
      "Shape after filtering results by relevant status IDs: (25373, 18)\n",
      "Starting initial merges...\n",
      "Merged qualifying info.\n",
      "Merged status info.\n",
      "Initial merges complete.\n",
      "Starting cleaning...\n",
      "Cleaned and filled NaN/0 grid positions with 25.\n",
      "Created base target variables.\n",
      "DataFrame sorted by date.\n",
      "--- STAGE 1 COMPLETE --- Shape after initial prep & filtering: (25373, 34) ---\n",
      "Unique Race IDs: 1125\n",
      "Unique Driver IDs: 814\n"
     ]
    }
   ],
   "source": [
    "# --- STAGE 1: Data Loading and Initial Preparation ---\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os # Import os for path checking\n",
    "\n",
    "print(\"--- STAGE 1 START ---\")\n",
    "\n",
    "# --- 1a. Load CSV files ---\n",
    "# <<< *** USER: Double-check this path is correct for your setup *** >>>\n",
    "data_path = '../data/f1_world_champion/' # Example: Assumes CSVs are in the same directory as the notebook\n",
    "print(f\"Using data_path: '{data_path}'\")\n",
    "\n",
    "# List of required files\n",
    "files_to_load = [\n",
    "    'results.csv', 'races.csv', 'drivers.csv', 'constructors.csv',\n",
    "    'qualifying.csv', 'circuits.csv', 'status.csv', 'constructor_results.csv',\n",
    "    'lap_times.csv', 'pit_stops.csv', 'sprint_results.csv'\n",
    "]\n",
    "\n",
    "# Dictionary to hold loaded dataframes\n",
    "dataframes = {}\n",
    "all_loaded = True\n",
    "\n",
    "for f_name in files_to_load:\n",
    "    f_path = os.path.join(data_path, f_name)\n",
    "    try:\n",
    "        dataframes[f_name.split('.')[0]] = pd.read_csv(f_path)\n",
    "        print(f\"Successfully loaded: {f_name} (Shape: {dataframes[f_name.split('.')[0]].shape})\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at '{f_path}'. Features requiring this file will be skipped.\")\n",
    "        dataframes[f_name.split('.')[0]] = None # Set to None if not found\n",
    "        all_loaded = False\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {f_name}: {e}\")\n",
    "        dataframes[f_name.split('.')[0]] = None\n",
    "        all_loaded = False\n",
    "\n",
    "# Assign to variables for convenience (optional, can use dataframes['results'] directly)\n",
    "results = dataframes.get('results')\n",
    "races = dataframes.get('races')\n",
    "drivers = dataframes.get('drivers')\n",
    "constructors = dataframes.get('constructors')\n",
    "qualifying = dataframes.get('qualifying')\n",
    "circuits = dataframes.get('circuits')\n",
    "status = dataframes.get('status')\n",
    "constructor_results = dataframes.get('constructor_results')\n",
    "lap_times = dataframes.get('lap_times')\n",
    "pit_stops = dataframes.get('pit_stops')\n",
    "sprint_results = dataframes.get('sprint_results')\n",
    "\n",
    "if not all_loaded:\n",
    "    print(\"\\nWarning: Not all required CSV files were loaded successfully.\")\n",
    "if results is None or races is None or status is None:\n",
    "    raise ValueError(\"Critical DataFrames ('results', 'races', 'status') could not be loaded. Aborting.\")\n",
    "\n",
    "# --- 1b. Define Relevant Status IDs for Filtering ---\n",
    "print(\"Defining relevant status IDs for analysis...\")\n",
    "# Categories 1(Acc/Inc), 2(Mech/Tech), 3(Drv/Team), 4(Procedural DNF - 2,62,92,96), 6(Finished/Lapped)\n",
    "# Excludes Cat 5 (DNS/DNQ/Safety/...)\n",
    "relevant_status_ids = [\n",
    "     # Cat 1\n",
    "     3, 4, 20, 104, 130, 137, 138,\n",
    "     # Cat 2\n",
    "     5, 6, 7, 8, 9, 10, 21, 22, 23, 24, 25, 26, 27, 29, 30, 32, 33, 34, 36, 37, 38, 39, 40, 41, 42, 43, 44, 46, 47, 48, 49, 51, 56, 59, 60, 61, 63, 64, 65, 66, 67, 69, 70, 71, 72, 74, 75, 76, 79, 80, 83, 84, 85, 86, 87, 91, 95, 98, 99, 101, 102, 103, 105, 106, 108, 109, 110, 121, 126, 129, 131, 132, 135, 140, 141,\n",
    "     # Cat 3\n",
    "     28, 31, 54, 68, 73, 82, 100, 107, 136, 139,\n",
    "     # Cat 4 (Selected)\n",
    "     2, 62, 92, 96,\n",
    "     # Cat 6\n",
    "     1, 11, 12, 13, 14, 15, 16, 17, 18, 19, 45, 50, 53, 55, 58, 88, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 122, 123, 124, 125, 127, 128, 133, 134\n",
    "]\n",
    "print(f\"Defined {len(relevant_status_ids)} relevant status IDs.\")\n",
    "\n",
    "# --- 1c. Filter Initial Results Data ---\n",
    "print(f\"Shape before filtering results: {results.shape}\")\n",
    "results_filtered = results[results['statusId'].isin(relevant_status_ids)].copy()\n",
    "print(f\"Shape after filtering results by relevant status IDs: {results_filtered.shape}\")\n",
    "\n",
    "\n",
    "# --- 1d. Initial Merges (using results_filtered) ---\n",
    "print(\"Starting initial merges...\")\n",
    "races_subset = races[['raceId', 'year', 'round', 'circuitId', 'date']]\n",
    "df = pd.merge(results_filtered, races_subset, on='raceId', how='left')\n",
    "\n",
    "# Merge other tables (checking if they were loaded)\n",
    "if drivers is not None:\n",
    "    drivers_subset = drivers[['driverId', 'driverRef', 'nationality', 'dob']]\n",
    "    df = pd.merge(df, drivers_subset, on='driverId', how='left')\n",
    "else: print(\"Skipping driver merge.\")\n",
    "if constructors is not None:\n",
    "    constructors_subset = constructors[['constructorId', 'name', 'nationality']]\n",
    "    constructors_subset = constructors_subset.rename(columns={'name': 'constructorName', 'nationality': 'constructorNationality'})\n",
    "    df = pd.merge(df, constructors_subset, on='constructorId', how='left')\n",
    "else: print(\"Skipping constructor merge.\")\n",
    "\n",
    "df = df.rename(columns={'grid': 'results_grid_start'}) # Rename original grid\n",
    "\n",
    "if qualifying is not None:\n",
    "    qualifying_subset = qualifying[['raceId', 'driverId', 'constructorId', 'position']].copy()\n",
    "    qualifying_subset['position'] = pd.to_numeric(qualifying_subset['position'], errors='coerce')\n",
    "    qualifying_subset = qualifying_subset.rename(columns={'position': 'grid'})\n",
    "    df = pd.merge(df, qualifying_subset, on=['raceId', 'driverId', 'constructorId'], how='left')\n",
    "    print(\"Merged qualifying info.\")\n",
    "else:\n",
    "    print(\"Warning: Qualifying DataFrame not loaded. Grid position feature will be NaN.\")\n",
    "    df['grid'] = np.nan\n",
    "\n",
    "if circuits is not None:\n",
    "    circuits_subset = circuits[['circuitId', 'name', 'location', 'country']]\n",
    "    circuits_subset = circuits_subset.rename(columns={'name': 'circuitName'})\n",
    "    df = pd.merge(df, circuits_subset, on='circuitId', how='left')\n",
    "else: print(\"Skipping circuit merge.\")\n",
    "\n",
    "if status is not None:\n",
    "    status_subset = status[['statusId', 'status']]\n",
    "    df = pd.merge(df, status_subset, on='statusId', how='left') # Status text added back\n",
    "    print(\"Merged status info.\")\n",
    "else:\n",
    "    df['status'] = 'Unknown'\n",
    "\n",
    "print(\"Initial merges complete.\")\n",
    "\n",
    "# --- 1e. Data Cleaning ---\n",
    "print(\"Starting cleaning...\")\n",
    "df_cleaned = df.copy() # Start with the merged, pre-filtered data\n",
    "\n",
    "# Type Conversions\n",
    "df_cleaned['date'] = pd.to_datetime(df_cleaned['date'], errors='coerce')\n",
    "if drivers is not None and 'dob' in df_cleaned.columns:\n",
    "    df_cleaned['dob'] = pd.to_datetime(df_cleaned['dob'], errors='coerce')\n",
    "\n",
    "# Handle grid NaNs/Zeros (e.g. pit lane start) -> Use a high number like 25\n",
    "if 'grid' in df_cleaned.columns:\n",
    "     df_cleaned['grid'] = pd.to_numeric(df_cleaned['grid'], errors='coerce').fillna(25) # Fill NaNs\n",
    "     df_cleaned.loc[df_cleaned['grid'] == 0, 'grid'] = 25 # Treat grid 0 as pit lane start (high number)\n",
    "     print(\"Cleaned and filled NaN/0 grid positions with 25.\")\n",
    "else:\n",
    "     print(\"Warning: 'grid' column missing.\")\n",
    "\n",
    "\n",
    "# --- 1f. Define RAW Target Variables ---\n",
    "if 'positionOrder' in df_cleaned.columns:\n",
    "    # Ensure positionOrder is numeric for target creation\n",
    "    df_cleaned['positionOrder'] = pd.to_numeric(df_cleaned['positionOrder'], errors='coerce')\n",
    "    df_cleaned['target_finishing_position'] = df_cleaned['positionOrder']\n",
    "    # Handle potential NaNs in positionOrder when defining podium finish\n",
    "    df_cleaned['target_podium_finish'] = df_cleaned['positionOrder'].apply(lambda x: 1 if pd.notna(x) and x <= 3 else 0)\n",
    "    print(\"Created base target variables.\")\n",
    "else:\n",
    "    print(\"Warning: 'positionOrder' column not found. Cannot create targets.\")\n",
    "    df_cleaned['target_finishing_position'] = np.nan\n",
    "    df_cleaned['target_podium_finish'] = 0\n",
    "\n",
    "\n",
    "# --- 1g. Sort DataFrame Chronologically ---\n",
    "df_cleaned.dropna(subset=['date'], inplace=True) # Drop rows if date conversion failed\n",
    "df_cleaned = df_cleaned.sort_values(by=['date', 'raceId']).reset_index(drop=True)\n",
    "print(\"DataFrame sorted by date.\")\n",
    "\n",
    "# --- Final Check for Block 1 ---\n",
    "print(f\"--- STAGE 1 COMPLETE --- Shape after initial prep & filtering: {df_cleaned.shape} ---\")\n",
    "# Check counts of key IDs\n",
    "print(f\"Unique Race IDs: {df_cleaned['raceId'].nunique()}\")\n",
    "print(f\"Unique Driver IDs: {df_cleaned['driverId'].nunique()}\")\n",
    "# Output of this block is df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9a8f7320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- STAGE 2 START ---\n",
      "Defined 96 status IDs as DNF indicators.\n",
      "Dropped potentially existing feature columns for fresh calculation.\n",
      "Created 'is_dnf' column.\n",
      "Engineered feature: 'avg_finish_pos_last_5' (driver).\n",
      "Engineered feature: 'avg_points_last_5' (constructor).\n",
      "Engineered feature: 'avg_finish_at_circuit' (driver).\n",
      "Calculating qualifying time difference from pole...\n",
      "Calculated 'quali_diff_from_pole_ms' for races with valid pole times.\n",
      "Filled NaNs in df_featured['quali_diff_from_pole_ms'] using the mean difference.\n",
      "Calculating teammate grid difference...\n",
      "Engineered feature: 'teammate_grid_diff'.\n",
      "Engineered feature: 'dnf_rate_last_5'.\n",
      "Calculating lap time history...\n",
      "Engineered feature: 'avg_lap_time_ms_last_5'.\n",
      "Calculating pit stop history...\n",
      "Engineered feature: 'avg_stops_last_5'.\n",
      "Incorporating sprint race data...\n",
      "Engineered features: 'sprint_pos', 'sprint_points', 'participated_in_sprint'.\n",
      "\n",
      "--- Checking NaNs after ALL Feature Engineering ---\n",
      "is_dnf                        0\n",
      "prev_finish_pos             814\n",
      "avg_finish_pos_last_5         0\n",
      "avg_points_last_5             0\n",
      "prev_finish_at_circuit     7659\n",
      "avg_finish_at_circuit         0\n",
      "quali_diff_from_pole_ms       0\n",
      "teammate_grid_diff            0\n",
      "prev_dnf                    814\n",
      "dnf_rate_last_5               0\n",
      "avg_lap_time_ms_last_5        0\n",
      "avg_stops_last_5              0\n",
      "sprint_pos                    0\n",
      "sprint_points                 0\n",
      "participated_in_sprint        0\n",
      "dtype: int64\n",
      "--- STAGE 2 COMPLETE --- Shape after feature engineering: (25373, 47) ---\n"
     ]
    }
   ],
   "source": [
    "# --- STAGE 2: Feature Engineering ---\n",
    "# This block adds calculated features to 'df_cleaned'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"--- STAGE 2 START ---\")\n",
    "# Make sure df_cleaned exists from Block 1\n",
    "if 'df_cleaned' not in locals() or df_cleaned is None:\n",
    "     raise NameError(\"DataFrame 'df_cleaned' not found. Please run Block 1 first.\")\n",
    "\n",
    "# Also check required DataFrames for specific features are loaded\n",
    "required_dfs = {'qualifying': qualifying, 'constructor_results': constructor_results,\n",
    "                'lap_times': lap_times, 'pit_stops': pit_stops,\n",
    "                'sprint_results': sprint_results, 'status': status, 'races': races}\n",
    "for name, df_check in required_dfs.items():\n",
    "    if df_check is None:\n",
    "        print(f\"Warning: DataFrame '{name}' is None, features requiring it will be skipped.\")\n",
    "\n",
    "\n",
    "# Create a working copy for feature engineering\n",
    "df_featured = df_cleaned.copy()\n",
    "\n",
    "# --- Define DNF Status IDs based on user categorization ---\n",
    "# Categories 1 (Accident/Incident), 2 (Mechanical/Technical), 3 (Driver/Team),\n",
    "# and selected from 4 (Disqualified, Not Classified, Excluded)\n",
    "dnf_status_ids = [\n",
    "     3, 4, 20, 104, 130, 137, 138, # Cat 1\n",
    "     5, 6, 7, 8, 9, 10, 21, 22, 23, 24, 25, 26, 27, 29, 30, 32, 33, 34, 36, 37, 38, 39, 40, 41, 42, 43, 44, 46, 47, 48, 49, 51, 56, 59, 60, 61, 63, 64, 65, 66, 67, 69, 70, 71, 72, 74, 75, 76, 79, 80, 83, 84, 85, 86, 87, 91, 95, 98, 99, 101, 102, 103, 105, 106, 108, 109, 110, 121, 126, 129, 131, 132, 135, 140, 141, # Cat 2 (Removed Fuel rig 71?) Check 135 Brake duct, 140 Undertray, 141 Cooling system\n",
    "     28, 31, 54, 68, 73, 82, 100, 107, 136, 139, # Cat 3\n",
    "     2, 62, 92, 96 # Cat 4 (Selected: Disqualified, Not classified, Underweight, Excluded)\n",
    "     # Excluded 35 (Refuelling) from Cat 4\n",
    " ]\n",
    "print(f\"Defined {len(dnf_status_ids)} status IDs as DNF indicators.\")\n",
    "\n",
    "# --- Drop potentially existing columns if re-running this cell ---\n",
    "# Add all potential new feature column names here\n",
    "cols_to_drop = [\n",
    "    'avg_finish_pos_last_5', 'prev_finish_pos', 'avg_points_last_5',\n",
    "    'prev_finish_at_circuit', 'avg_finish_at_circuit', 'quali_diff_from_pole_ms',\n",
    "    'teammate_grid_diff', 'is_dnf', 'prev_dnf', 'dnf_rate_last_5', # Changed from dnf_last_5_races\n",
    "    'constructor_dnf_rate_last_5', 'avg_lap_time_ms_last_5', 'prev_avg_lap_time',\n",
    "    'avg_stops_last_5', 'prev_num_stops', 'sprint_pos', 'sprint_points', 'participated_in_sprint'\n",
    "]\n",
    "df_featured = df_featured.drop(columns=cols_to_drop, errors='ignore')\n",
    "print(\"Dropped potentially existing feature columns for fresh calculation.\")\n",
    "\n",
    "# --- Feature Engineering Calculations ---\n",
    "\n",
    "# Base features needed downstream\n",
    "if 'statusId' in df_featured.columns:\n",
    "    df_featured['is_dnf'] = df_featured['statusId'].apply(lambda x: 1 if x in dnf_status_ids else 0)\n",
    "    print(\"Created 'is_dnf' column.\")\n",
    "else:\n",
    "    df_featured['is_dnf'] = 0 # Assume not DNF if status unknown\n",
    "    print(\"Warning: 'statusId' missing, cannot calculate 'is_dnf'. Set to 0.\")\n",
    "\n",
    "# 2a. Driver Historical Performance (Avg Finish Last 5)\n",
    "if 'target_finishing_position' in df_featured.columns:\n",
    "    df_featured['prev_finish_pos'] = df_featured.groupby('driverId')['target_finishing_position'].shift(1)\n",
    "    df_featured['avg_finish_pos_last_5'] = df_featured.groupby('driverId')['prev_finish_pos'].transform(\n",
    "        lambda x: x.rolling(window=5, min_periods=1).mean()\n",
    "    )\n",
    "    mean_overall_finish = df_featured['target_finishing_position'].mean()\n",
    "    df_featured['avg_finish_pos_last_5'] = df_featured['avg_finish_pos_last_5'].fillna(mean_overall_finish)\n",
    "    print(\"Engineered feature: 'avg_finish_pos_last_5' (driver).\")\n",
    "else: print(\"Skipping 'avg_finish_pos_last_5': target missing.\")\n",
    "\n",
    "# 2b. Constructor Historical Performance (Avg Points Last 5)\n",
    "if constructor_results is not None and races is not None:\n",
    "    try:\n",
    "        races_minimal = races[['raceId', 'date']]\n",
    "        constructor_results_merged = pd.merge(constructor_results.dropna(subset=['points']), races_minimal, on='raceId', how='left').dropna(subset=['date'])\n",
    "        constructor_results_merged = constructor_results_merged.sort_values(by=['date', 'raceId'])\n",
    "        constructor_results_merged['prev_points'] = constructor_results_merged.groupby('constructorId')['points'].shift(1)\n",
    "        constructor_results_merged['avg_points_last_5'] = constructor_results_merged.groupby('constructorId')['prev_points'].transform(\n",
    "            lambda x: x.rolling(window=5, min_periods=1).mean()\n",
    "        ).fillna(0) # Fill NaNs within transform\n",
    "        constructor_hist_points = constructor_results_merged[['raceId', 'constructorId', 'avg_points_last_5']].drop_duplicates(['raceId', 'constructorId'], keep='last') # Ensure unique race/constructor combo\n",
    "        df_featured = pd.merge(df_featured, constructor_hist_points, on=['raceId', 'constructorId'], how='left')\n",
    "        df_featured['avg_points_last_5'] = df_featured['avg_points_last_5'].fillna(0)\n",
    "        print(\"Engineered feature: 'avg_points_last_5' (constructor).\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating constructor points history: {e}\")\n",
    "        df_featured['avg_points_last_5'] = 0 # Default value on error\n",
    "else:\n",
    "    print(\"Skipping constructor avg points: required DataFrames missing.\")\n",
    "    df_featured['avg_points_last_5'] = 0\n",
    "\n",
    "\n",
    "# 2c. Driver Performance at Specific Circuit (Avg Finish Pos)\n",
    "if 'target_finishing_position' in df_featured.columns:\n",
    "    df_featured['prev_finish_at_circuit'] = df_featured.groupby(['driverId', 'circuitId'])['target_finishing_position'].shift(1)\n",
    "    df_featured['avg_finish_at_circuit'] = df_featured.groupby(['driverId', 'circuitId'])['prev_finish_at_circuit'].transform(\n",
    "        lambda x: x.expanding(min_periods=1).mean()\n",
    "    )\n",
    "    if 'mean_overall_finish' not in locals(): mean_overall_finish = df_featured['target_finishing_position'].mean()\n",
    "    df_featured['avg_finish_at_circuit'] = df_featured['avg_finish_at_circuit'].fillna(mean_overall_finish)\n",
    "    print(\"Engineered feature: 'avg_finish_at_circuit' (driver).\")\n",
    "else:\n",
    "    print(\"Skipping driver avg finish at circuit: target missing.\")\n",
    "    df_featured['avg_finish_at_circuit'] = mean_overall_finish if 'mean_overall_finish' in locals() else 10 # Default\n",
    "\n",
    "\n",
    "# --- 2d. Qualifying Details (Time Difference from Pole) ---\n",
    "\n",
    "# Function to parse time string MM:SS.ms to milliseconds (keep definition here for clarity)\n",
    "def time_to_millis(time_str):\n",
    "    if pd.isna(time_str) or time_str == '\\\\N': return np.nan\n",
    "    if isinstance(time_str, str) and not time_str.strip(): return np.nan\n",
    "    try:\n",
    "        parts = str(time_str).split(':')\n",
    "        if len(parts) == 2: # MM:SS.ms\n",
    "            minutes = int(parts[0]); seconds_parts = parts[1].split('.'); seconds = int(seconds_parts[0]); millis = int(seconds_parts[1])\n",
    "            return (minutes * 60 + seconds) * 1000 + millis\n",
    "        elif '.' in str(time_str): # SS.ms\n",
    "             seconds_parts = str(time_str).split('.'); seconds = int(seconds_parts[0]); millis = int(seconds_parts[1])\n",
    "             return seconds * 1000 + millis\n",
    "        else: return int(time_str)*1000 # Assume whole seconds\n",
    "    except Exception: return np.nan\n",
    "\n",
    "# Check if qualifying DataFrame exists\n",
    "if 'qualifying' in locals() and qualifying is not None:\n",
    "    try:\n",
    "        print(\"Calculating qualifying time difference from pole...\")\n",
    "        qualifying_copy = qualifying.copy() # Work on a copy\n",
    "\n",
    "        # Apply time conversion\n",
    "        qualifying_copy['q1_millis'] = qualifying_copy['q1'].apply(time_to_millis)\n",
    "        qualifying_copy['q2_millis'] = qualifying_copy['q2'].apply(time_to_millis)\n",
    "        qualifying_copy['q3_millis'] = qualifying_copy['q3'].apply(time_to_millis)\n",
    "        qualifying_copy['best_quali_time_millis'] = qualifying_copy['q3_millis'].fillna(qualifying_copy['q2_millis']).fillna(qualifying_copy['q1_millis'])\n",
    "\n",
    "        # Find indices of minimum times per race using idxmin()\n",
    "        min_time_indices = qualifying_copy.groupby('raceId')['best_quali_time_millis'].idxmin()\n",
    "\n",
    "        # Filter out any NaN indices that resulted from races with no valid times\n",
    "        valid_indices = min_time_indices.dropna().astype(int) # Ensure integer indices\n",
    "\n",
    "        # Use only the valid indices to select the rows corresponding to pole times\n",
    "        if not valid_indices.empty:\n",
    "            # Check if all valid indices exist in the qualifying_copy index\n",
    "            valid_indices = valid_indices[valid_indices.isin(qualifying_copy.index)]\n",
    "            if not valid_indices.empty:\n",
    "                pole_times = qualifying_copy.loc[valid_indices, ['raceId', 'best_quali_time_millis']].rename(\n",
    "                    columns={'best_quali_time_millis': 'pole_time_millis'}\n",
    "                )\n",
    "                # Merge pole times into the qualifying_copy DataFrame using a left merge\n",
    "                qualifying_copy = pd.merge(qualifying_copy, pole_times, on='raceId', how='left')\n",
    "\n",
    "                # Now calculate the difference; result is NaN if 'pole_time_millis' is NaN for that row\n",
    "                qualifying_copy['quali_diff_from_pole_ms'] = qualifying_copy['best_quali_time_millis'] - qualifying_copy['pole_time_millis']\n",
    "                print(\"Calculated 'quali_diff_from_pole_ms' for races with valid pole times.\")\n",
    "            else:\n",
    "                 # If valid_indices become empty after checking against index (shouldn't normally happen with idxmin unless data is strange)\n",
    "                 qualifying_copy['quali_diff_from_pole_ms'] = np.nan\n",
    "                 print(\"Warning: Valid pole time indices were not found in the DataFrame index. 'quali_diff_from_pole_ms' column created with NaNs.\")\n",
    "        else:\n",
    "            # If no valid pole times found at all, create the column with NaNs\n",
    "            qualifying_copy['quali_diff_from_pole_ms'] = np.nan\n",
    "            print(\"Warning: No valid pole times found. 'quali_diff_from_pole_ms' column created with NaNs.\")\n",
    "\n",
    "        # Now, regardless of the if/else, the column 'quali_diff_from_pole_ms' should exist\n",
    "        # Select data for final merge, ensuring the column exists first\n",
    "        if 'quali_diff_from_pole_ms' in qualifying_copy.columns:\n",
    "            quali_diff_data = qualifying_copy[['raceId', 'driverId', 'constructorId', 'quali_diff_from_pole_ms']].drop_duplicates(['raceId', 'driverId', 'constructorId'], keep='last')\n",
    "            # Merge into df_featured\n",
    "            df_featured = pd.merge(df_featured, quali_diff_data, on=['raceId', 'driverId', 'constructorId'], how='left')\n",
    "\n",
    "            # Fill any remaining NaNs in the final column in df_featured\n",
    "            if 'quali_diff_from_pole_ms' in df_featured.columns and df_featured['quali_diff_from_pole_ms'].notna().any():\n",
    "                mean_quali_diff = df_featured['quali_diff_from_pole_ms'].mean()\n",
    "                df_featured['quali_diff_from_pole_ms'] = df_featured['quali_diff_from_pole_ms'].fillna(mean_quali_diff)\n",
    "                print(\"Filled NaNs in df_featured['quali_diff_from_pole_ms'] using the mean difference.\")\n",
    "            elif 'quali_diff_from_pole_ms' in df_featured.columns:\n",
    "                 # Handle case where column exists but is all NaN after merge\n",
    "                 df_featured['quali_diff_from_pole_ms'] = df_featured['quali_diff_from_pole_ms'].fillna(999999) # Example fill with large value\n",
    "                 print(\"Filled NaNs in df_featured['quali_diff_from_pole_ms'] using a default large value (999999).\")\n",
    "        else:\n",
    "             # Fallback if column wasn't created properly\n",
    "             print(\"Warning: 'quali_diff_from_pole_ms' column missing in qualifying_copy before final merge. Setting default in df_featured.\")\n",
    "             df_featured['quali_diff_from_pole_ms'] = 999999\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating qualifying diff: {e}\")\n",
    "        # Ensure column exists even if error occurs, fill with default\n",
    "        if 'quali_diff_from_pole_ms' not in df_featured.columns:\n",
    "             df_featured['quali_diff_from_pole_ms'] = 999999\n",
    "        else: # If column exists from merge but error happened during fill\n",
    "             df_featured['quali_diff_from_pole_ms'] = df_featured['quali_diff_from_pole_ms'].fillna(999999)\n",
    "\n",
    "else:\n",
    "    print(\"Skipping qualifying diff feature: qualifying DataFrame missing.\")\n",
    "    # Ensure column exists for consistency downstream, fill with default\n",
    "    df_featured['quali_diff_from_pole_ms'] = 999999\n",
    "\n",
    "\n",
    "# 2e. Teammate Comparison (Grid Position Difference)\n",
    "if 'grid' in df_featured.columns:\n",
    "    try:\n",
    "        print(\"Calculating teammate grid difference...\")\n",
    "        teammate_data = df_featured[['raceId', 'constructorId', 'driverId', 'grid']].rename(\n",
    "            columns={'grid': 'teammate_grid', 'driverId': 'teammate_driverId'}\n",
    "        )\n",
    "        df_merged_teammates = pd.merge(df_featured[['raceId', 'driverId', 'constructorId', 'grid']], teammate_data, on=['raceId', 'constructorId'], how='left')\n",
    "        df_merged_teammates = df_merged_teammates[df_merged_teammates['driverId'] != df_merged_teammates['teammate_driverId']]\n",
    "        df_merged_teammates['grid_diff'] = df_merged_teammates['grid'] - df_merged_teammates['teammate_grid']\n",
    "        teammate_grid_diff = df_merged_teammates.groupby(['raceId', 'driverId'])['grid_diff'].mean().reset_index().rename(columns={'grid_diff': 'teammate_grid_diff'})\n",
    "        df_featured = pd.merge(df_featured, teammate_grid_diff, on=['raceId', 'driverId'], how='left')\n",
    "        df_featured['teammate_grid_diff'] = df_featured['teammate_grid_diff'].fillna(0) # Fill NaNs (no teammate) with 0 diff\n",
    "        print(\"Engineered feature: 'teammate_grid_diff'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating teammate grid diff: {e}\")\n",
    "        df_featured['teammate_grid_diff'] = 0\n",
    "else:\n",
    "    print(\"Skipping teammate grid diff: 'grid' column missing.\")\n",
    "    df_featured['teammate_grid_diff'] = 0\n",
    "\n",
    "\n",
    "# 2f. Granular History (DNF Rate Last 5 Races)\n",
    "if 'is_dnf' in df_featured.columns:\n",
    "    df_featured['prev_dnf'] = df_featured.groupby('driverId')['is_dnf'].shift(1)\n",
    "    df_featured['dnf_rate_last_5'] = df_featured.groupby('driverId')['prev_dnf'].transform(\n",
    "        lambda x: x.rolling(window=5, min_periods=0).mean() # Rate instead of count\n",
    "    ).fillna(0)\n",
    "    print(\"Engineered feature: 'dnf_rate_last_5'.\")\n",
    "else:\n",
    "    print(\"Skipping DNF rate feature: 'is_dnf' column missing.\")\n",
    "    df_featured['dnf_rate_last_5'] = 0\n",
    "\n",
    "\n",
    "# --- Add Constructor DNF Rate (Optional but potentially useful) ---\n",
    "# ... (Similar logic to driver DNF rate but grouping by constructor) ...\n",
    "# --- Skipping for now to keep complexity manageable ---\n",
    "\n",
    "\n",
    "# 2g. Lap Time Data (Avg Lap Time ms Last 5 Races)\n",
    "if lap_times is not None and races is not None:\n",
    "    try:\n",
    "        print(\"Calculating lap time history...\")\n",
    "        # Filter lap times for only drivers/races in our main df\n",
    "        relevant_races = df_featured['raceId'].unique()\n",
    "        relevant_drivers = df_featured['driverId'].unique()\n",
    "        laptimes_filtered = lap_times[lap_times['raceId'].isin(relevant_races) & lap_times['driverId'].isin(relevant_drivers)].copy()\n",
    "        laptimes_filtered['milliseconds'] = pd.to_numeric(laptimes_filtered['milliseconds'], errors='coerce')\n",
    "        laptimes_filtered.dropna(subset=['milliseconds'], inplace=True)\n",
    "\n",
    "        avg_lap_times = laptimes_filtered.groupby(['raceId', 'driverId'])['milliseconds'].mean().reset_index()\n",
    "        avg_lap_times = avg_lap_times.rename(columns={'milliseconds': 'avg_lap_time_ms'})\n",
    "        avg_lap_times = pd.merge(avg_lap_times, races[['raceId', 'date']], on='raceId', how='left').dropna(subset=['date'])\n",
    "        avg_lap_times = avg_lap_times.sort_values(by=['date', 'raceId'])\n",
    "        avg_lap_times['prev_avg_lap_time'] = avg_lap_times.groupby('driverId')['avg_lap_time_ms'].shift(1)\n",
    "        avg_lap_times['avg_lap_time_ms_last_5'] = avg_lap_times.groupby('driverId')['prev_avg_lap_time'].transform(\n",
    "            lambda x: x.rolling(window=5, min_periods=1).mean()\n",
    "        )\n",
    "        mean_overall_lap_time = avg_lap_times['avg_lap_time_ms'].mean() # Calculate mean before filling NaNs\n",
    "        avg_lap_times['avg_lap_time_ms_last_5'] = avg_lap_times['avg_lap_time_ms_last_5'].fillna(mean_overall_lap_time)\n",
    "\n",
    "        lap_time_history = avg_lap_times[['raceId', 'driverId', 'avg_lap_time_ms_last_5']].drop_duplicates(['raceId','driverId'], keep='last')\n",
    "        df_featured = pd.merge(df_featured, lap_time_history, on=['raceId', 'driverId'], how='left')\n",
    "        df_featured['avg_lap_time_ms_last_5'] = df_featured['avg_lap_time_ms_last_5'].fillna(mean_overall_lap_time)\n",
    "        print(\"Engineered feature: 'avg_lap_time_ms_last_5'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating lap time history: {e}\")\n",
    "        df_featured['avg_lap_time_ms_last_5'] = df_featured['avg_lap_time_ms_last_5'].mean() if 'avg_lap_time_ms_last_5' in df_featured else 95000 # Default backup\n",
    "else:\n",
    "    print(\"Skipping lap time history: lap_times or races DataFrame missing.\")\n",
    "    df_featured['avg_lap_time_ms_last_5'] = 95000 # Approx default ms\n",
    "\n",
    "\n",
    "# 2h. Pit Stop Data (Avg Stops Last 5 Races)\n",
    "if pit_stops is not None and races is not None:\n",
    "    try:\n",
    "        print(\"Calculating pit stop history...\")\n",
    "        num_stops = pit_stops.groupby(['raceId', 'driverId'])['stop'].max().reset_index()\n",
    "        num_stops = num_stops.rename(columns={'stop': 'num_pit_stops'})\n",
    "        num_stops = pd.merge(num_stops, races[['raceId', 'date']], on='raceId', how='left').dropna(subset=['date'])\n",
    "        num_stops = num_stops.sort_values(by=['date', 'raceId'])\n",
    "        num_stops['prev_num_stops'] = num_stops.groupby('driverId')['num_pit_stops'].shift(1)\n",
    "        num_stops['avg_stops_last_5'] = num_stops.groupby('driverId')['prev_num_stops'].transform(\n",
    "            lambda x: x.rolling(window=5, min_periods=0).mean()\n",
    "        ).fillna(0)\n",
    "\n",
    "        pit_stop_history = num_stops[['raceId', 'driverId', 'avg_stops_last_5']].drop_duplicates(['raceId','driverId'], keep='last')\n",
    "        df_featured = pd.merge(df_featured, pit_stop_history, on=['raceId', 'driverId'], how='left')\n",
    "        df_featured['avg_stops_last_5'] = df_featured['avg_stops_last_5'].fillna(0) # Fill drivers with no stops in history with 0\n",
    "        print(\"Engineered feature: 'avg_stops_last_5'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating pit stop history: {e}\")\n",
    "        df_featured['avg_stops_last_5'] = 1 # Default fallback\n",
    "else:\n",
    "    print(\"Skipping pit stop history: pit_stops or races DataFrame missing.\")\n",
    "    df_featured['avg_stops_last_5'] = 1\n",
    "\n",
    "\n",
    "# 2i. Sprint Race Data\n",
    "if sprint_results is not None:\n",
    "    try:\n",
    "        print(\"Incorporating sprint race data...\")\n",
    "        sprint_data = sprint_results[['raceId', 'driverId', 'positionOrder', 'points']].copy()\n",
    "        sprint_data = sprint_data.rename(columns={'positionOrder': 'sprint_pos', 'points': 'sprint_points'})\n",
    "        df_featured = pd.merge(df_featured, sprint_data, on=['raceId', 'driverId'], how='left')\n",
    "        df_featured['participated_in_sprint'] = df_featured['sprint_pos'].notna().astype(int)\n",
    "        # Fill NaNs for non-sprint races or drivers not in sprint\n",
    "        df_featured['sprint_pos'] = df_featured['sprint_pos'].fillna(25) # High value for position\n",
    "        df_featured['sprint_points'] = df_featured['sprint_points'].fillna(0) # 0 points\n",
    "        print(\"Engineered features: 'sprint_pos', 'sprint_points', 'participated_in_sprint'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error incorporating sprint data: {e}\")\n",
    "        df_featured['sprint_pos'] = 25\n",
    "        df_featured['sprint_points'] = 0\n",
    "        df_featured['participated_in_sprint'] = 0\n",
    "else:\n",
    "    print(\"Skipping sprint results data: sprint_results DataFrame missing.\")\n",
    "    df_featured['sprint_pos'] = 25\n",
    "    df_featured['sprint_points'] = 0\n",
    "    df_featured['participated_in_sprint'] = 0\n",
    "\n",
    "\n",
    "# --- Final Check ---\n",
    "print(\"\\n--- Checking NaNs after ALL Feature Engineering ---\")\n",
    "final_feature_cols = [col for col in df_featured.columns if col not in df_cleaned.columns or col in ['avg_finish_pos_last_5', 'avg_points_last_5', 'avg_finish_at_circuit', 'quali_diff_from_pole_ms', 'teammate_grid_diff', 'dnf_rate_last_5', 'avg_lap_time_ms_last_5', 'avg_stops_last_5', 'sprint_pos', 'sprint_points', 'participated_in_sprint']]\n",
    "final_feature_cols = [col for col in final_feature_cols if col in df_featured.columns] # Ensure they exist\n",
    "if final_feature_cols:\n",
    "    print(df_featured[final_feature_cols].isnull().sum())\n",
    "else:\n",
    "    print(\"No new feature columns found to check for NaNs.\")\n",
    "\n",
    "# Optional: Clean intermediate columns if needed (e.g., 'prev_finish_pos')\n",
    "df_featured = df_featured.drop(columns=['prev_finish_pos', 'prev_finish_at_circuit'], errors='ignore')\n",
    "\n",
    "\n",
    "print(f\"--- STAGE 2 COMPLETE --- Shape after feature engineering: {df_featured.shape} ---\")\n",
    "# The final DataFrame with all features is df_featured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ca48a7e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- STAGE 3 START ---\n",
      "Features selected for modeling: ['grid', 'year', 'avg_finish_pos_last_5', 'avg_points_last_5', 'avg_finish_at_circuit', 'quali_diff_from_pole_ms', 'teammate_grid_diff', 'dnf_rate_last_5', 'avg_lap_time_ms_last_5', 'avg_stops_last_5', 'sprint_pos', 'sprint_points', 'driverId', 'constructorId', 'circuitId', 'participated_in_sprint']\n",
      "Data split complete. Train Shape: (20298, 16), Test Shape: (5075, 16)\n",
      "Numerical columns for pipeline: ['grid', 'year', 'avg_finish_pos_last_5', 'avg_points_last_5', 'avg_finish_at_circuit', 'quali_diff_from_pole_ms', 'teammate_grid_diff', 'dnf_rate_last_5', 'avg_lap_time_ms_last_5', 'avg_stops_last_5', 'sprint_pos', 'sprint_points', 'driverId', 'constructorId', 'circuitId', 'participated_in_sprint']\n",
      "Categorical columns for pipeline: ['driverId', 'constructorId', 'circuitId', 'participated_in_sprint']\n",
      "Preprocessor defined.\n",
      "--- STAGE 3 COMPLETE ---\n"
     ]
    }
   ],
   "source": [
    "# --- STAGE 3: Define Features, Targets, Split Data, Define Preprocessor ---\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Keep necessary imports for this block\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "print(\"--- STAGE 3 START ---\")\n",
    "# Make sure df_featured exists from Block 2\n",
    "if 'df_featured' not in locals() or df_featured is None:\n",
    "     raise NameError(\"DataFrame 'df_featured' not found. Please run Block 1 and 2 first.\")\n",
    "\n",
    "# --- 3a. Define Features and Targets ---\n",
    "# ** IMPORTANT: Ensure this list matches columns created in Block 2 **\n",
    "categorical_features = [\n",
    "    'driverId',\n",
    "    'constructorId',\n",
    "    'circuitId',\n",
    "    'participated_in_sprint' # Added as categorical/flag\n",
    "]\n",
    "numerical_features = [\n",
    "    'grid',\n",
    "    'year',\n",
    "    'avg_finish_pos_last_5',\n",
    "    'avg_points_last_5',\n",
    "    'avg_finish_at_circuit',\n",
    "    'quali_diff_from_pole_ms',\n",
    "    'teammate_grid_diff',\n",
    "    'dnf_rate_last_5',\n",
    "    'avg_lap_time_ms_last_5',\n",
    "    'avg_stops_last_5',\n",
    "    'sprint_pos',\n",
    "    'sprint_points'\n",
    "]\n",
    "\n",
    "target_regr = 'target_finishing_position'\n",
    "target_clas = 'target_podium_finish'\n",
    "\n",
    "# Check if all defined features and targets exist\n",
    "all_needed_columns = numerical_features + categorical_features + [target_regr, target_clas]\n",
    "missing_cols = [col for col in all_needed_columns if col not in df_featured.columns]\n",
    "if missing_cols:\n",
    "    raise ValueError(f\"Missing required columns in df_featured: {missing_cols}.\")\n",
    "\n",
    "# Select final features (X) and targets (y)\n",
    "features = numerical_features + categorical_features\n",
    "X = df_featured[features].copy()\n",
    "y_regr = df_featured[target_regr]\n",
    "y_clas = df_featured[target_clas]\n",
    "print(f\"Features selected for modeling: {features}\")\n",
    "\n",
    "# Handle potential NaNs in X\n",
    "if X.isnull().sum().sum() > 0:\n",
    "    print(\"Warning: NaNs found in final feature set X. Imputing with 0.\")\n",
    "    X = X.fillna(0)\n",
    "\n",
    "# --- 3b. Data Splitting ---\n",
    "# We split ONCE here. Optuna will use X_train, y_train for cross-validation.\n",
    "# X_test, y_test are held out for FINAL evaluation ONLY.\n",
    "X_train, X_test, y_train_regr, y_test_regr = train_test_split(\n",
    "    X, y_regr, test_size=0.2, random_state=42, stratify=df_featured['year']\n",
    ")\n",
    "y_train_clas = y_clas.loc[X_train.index]\n",
    "y_test_clas = y_clas.loc[X_test.index]\n",
    "print(f\"Data split complete. Train Shape: {X_train.shape}, Test Shape: {X_test.shape}\")\n",
    "\n",
    "# --- 3c. Define Preprocessor (Used in Tuning and Final Model) ---\n",
    "numerical_cols_final = X_train.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_cols_final = [col for col in categorical_features if col in X_train.columns] # Use predefined list\n",
    "\n",
    "print(f\"Numerical columns for pipeline: {numerical_cols_final}\")\n",
    "print(f\"Categorical columns for pipeline: {categorical_cols_final}\")\n",
    "\n",
    "# Define the preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_cols_final),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_cols_final)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "print(\"Preprocessor defined.\")\n",
    "\n",
    "# --- REMOVED: Pipeline definitions, fitting, and evaluation on test set ---\n",
    "# --- Those steps are moved to Block 5, after tuning ---\n",
    "\n",
    "print(\"--- STAGE 3 COMPLETE ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b539a603",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-06 15:04:25,323] A new study created in memory with name: no-name-27d0b3b1-8ecb-444d-abcf-cbfa58073d47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- STAGE 4 START ---\n",
      "\n",
      "--- Running XGBoost Tuning for Regression ---\n",
      "Starting Optuna study for XGBoost Regression (MAE, 50 trials, 5-fold CV)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-06 15:04:36,945] Trial 0 finished with value: -4.815522003173828 and parameters: {'n_estimators': 400, 'learning_rate': 0.11284802133425699, 'max_depth': 5, 'subsample': 0.9456040052527537, 'colsample_bytree': 0.5696922940375366, 'gamma': 1.0200399378811364, 'reg_alpha': 0.0021560960788677857, 'reg_lambda': 0.0033851780773969213}. Best is trial 0 with value: -4.815522003173828.\n",
      "[I 2025-04-06 15:04:42,642] Trial 1 finished with value: -4.907850074768066 and parameters: {'n_estimators': 100, 'learning_rate': 0.10492842394256792, 'max_depth': 3, 'subsample': 0.6736783727927032, 'colsample_bytree': 0.7508047613125373, 'gamma': 2.501727098948248, 'reg_alpha': 0.5406923056646545, 'reg_lambda': 0.000932662723573297}. Best is trial 0 with value: -4.815522003173828.\n",
      "[I 2025-04-06 15:05:13,618] Trial 2 finished with value: -4.942002391815185 and parameters: {'n_estimators': 1000, 'learning_rate': 0.1006941049639469, 'max_depth': 9, 'subsample': 0.5979461629352489, 'colsample_bytree': 0.7075854842764817, 'gamma': 4.767062870997184, 'reg_alpha': 0.019819613742670245, 'reg_lambda': 5.404168631598815e-06}. Best is trial 0 with value: -4.815522003173828.\n",
      "[I 2025-04-06 15:05:19,341] Trial 3 finished with value: -4.871236515045166 and parameters: {'n_estimators': 100, 'learning_rate': 0.22082493250050306, 'max_depth': 3, 'subsample': 0.7334191211908407, 'colsample_bytree': 0.6142131306349028, 'gamma': 3.3036730944773405, 'reg_alpha': 3.934748308066098e-06, 'reg_lambda': 0.029560029449226248}. Best is trial 0 with value: -4.815522003173828.\n",
      "[I 2025-04-06 15:05:42,151] Trial 4 finished with value: -4.874228763580322 and parameters: {'n_estimators': 800, 'learning_rate': 0.08849677157215002, 'max_depth': 7, 'subsample': 0.512998788326724, 'colsample_bytree': 0.9295614565764372, 'gamma': 3.7856738531579093, 'reg_alpha': 0.07865639740887093, 'reg_lambda': 0.36021243445076623}. Best is trial 0 with value: -4.815522003173828.\n",
      "[I 2025-04-06 15:05:54,805] Trial 5 finished with value: -4.845320034027099 and parameters: {'n_estimators': 300, 'learning_rate': 0.017504505335339704, 'max_depth': 6, 'subsample': 0.7834652456509692, 'colsample_bytree': 0.9036253549539381, 'gamma': 4.883291321714628, 'reg_alpha': 3.170243135275493e-05, 'reg_lambda': 0.28189875492925026}. Best is trial 0 with value: -4.815522003173828.\n",
      "[I 2025-04-06 15:06:26,463] Trial 6 finished with value: -4.806733798980713 and parameters: {'n_estimators': 900, 'learning_rate': 0.015706856094385636, 'max_depth': 9, 'subsample': 0.8287824072396568, 'colsample_bytree': 0.5010584004280779, 'gamma': 4.7087940894383475, 'reg_alpha': 0.000526552723040898, 'reg_lambda': 5.56889479665216e-06}. Best is trial 6 with value: -4.806733798980713.\n",
      "[I 2025-04-06 15:06:59,034] Trial 7 finished with value: -4.810488605499268 and parameters: {'n_estimators': 1000, 'learning_rate': 0.01164026426140872, 'max_depth': 7, 'subsample': 0.9287991890761087, 'colsample_bytree': 0.9205266078048019, 'gamma': 0.7223222703733767, 'reg_alpha': 1.2377713500918633e-07, 'reg_lambda': 7.732949963048657e-08}. Best is trial 6 with value: -4.806733798980713.\n",
      "[I 2025-04-06 15:07:12,253] Trial 8 finished with value: -4.8091309547424315 and parameters: {'n_estimators': 400, 'learning_rate': 0.036005928068727586, 'max_depth': 6, 'subsample': 0.8396961125240653, 'colsample_bytree': 0.5910857793886525, 'gamma': 1.5174186284163889, 'reg_alpha': 0.2983374215361807, 'reg_lambda': 2.4399723911698408e-08}. Best is trial 6 with value: -4.806733798980713.\n",
      "[I 2025-04-06 15:07:26,580] Trial 9 finished with value: -4.843037700653076 and parameters: {'n_estimators': 600, 'learning_rate': 0.1105431601336561, 'max_depth': 3, 'subsample': 0.6946348733755563, 'colsample_bytree': 0.649540326324169, 'gamma': 4.107336952203522, 'reg_alpha': 3.766573571061835e-08, 'reg_lambda': 1.2802083974241677e-08}. Best is trial 6 with value: -4.806733798980713.\n",
      "[I 2025-04-06 15:07:52,015] Trial 10 finished with value: -4.816276550292969 and parameters: {'n_estimators': 700, 'learning_rate': 0.026133969954280852, 'max_depth': 10, 'subsample': 0.8513842371168655, 'colsample_bytree': 0.5180913075456933, 'gamma': 2.371297223400725, 'reg_alpha': 0.0009965810930910042, 'reg_lambda': 1.0935583008480605e-05}. Best is trial 6 with value: -4.806733798980713.\n",
      "[I 2025-04-06 15:08:05,877] Trial 11 finished with value: -4.802155876159668 and parameters: {'n_estimators': 400, 'learning_rate': 0.0397067037680892, 'max_depth': 8, 'subsample': 0.8386134187986036, 'colsample_bytree': 0.5191912020311743, 'gamma': 1.7901930087088558, 'reg_alpha': 0.0002482658804700076, 'reg_lambda': 5.632095661843904e-07}. Best is trial 11 with value: -4.802155876159668.\n",
      "[I 2025-04-06 15:08:30,257] Trial 12 finished with value: -4.817038440704346 and parameters: {'n_estimators': 800, 'learning_rate': 0.043561230389305164, 'max_depth': 9, 'subsample': 0.8640475635810348, 'colsample_bytree': 0.5173718695382317, 'gamma': 2.006097020649662, 'reg_alpha': 0.00011380068748890795, 'reg_lambda': 1.0552258834386672e-06}. Best is trial 11 with value: -4.802155876159668.\n",
      "[I 2025-04-06 15:08:53,677] Trial 13 finished with value: -4.8271533966064455 and parameters: {'n_estimators': 500, 'learning_rate': 0.010132550099745608, 'max_depth': 8, 'subsample': 0.7940857102113827, 'colsample_bytree': 0.8087172183828961, 'gamma': 2.9659548288047484, 'reg_alpha': 2.7953366668471964e-06, 'reg_lambda': 0.00015120334724463738}. Best is trial 11 with value: -4.802155876159668.\n",
      "[I 2025-04-06 15:09:10,444] Trial 14 finished with value: -4.8232629776000975 and parameters: {'n_estimators': 300, 'learning_rate': 0.02134596230338743, 'max_depth': 10, 'subsample': 0.8998895393038424, 'colsample_bytree': 0.5059841787536872, 'gamma': 0.12022333350224512, 'reg_alpha': 0.0024426102640836447, 'reg_lambda': 5.122890972912441e-07}. Best is trial 11 with value: -4.802155876159668.\n",
      "[I 2025-04-06 15:09:33,228] Trial 15 finished with value: -4.835877799987793 and parameters: {'n_estimators': 800, 'learning_rate': 0.05559472216212872, 'max_depth': 8, 'subsample': 0.9920945575498763, 'colsample_bytree': 0.6626630691793649, 'gamma': 1.4110587588543124, 'reg_alpha': 0.00010417079066983409, 'reg_lambda': 3.313143019821431e-05}. Best is trial 11 with value: -4.802155876159668.\n",
      "[I 2025-04-06 15:09:55,936] Trial 16 finished with value: -4.80582628250122 and parameters: {'n_estimators': 600, 'learning_rate': 0.016846887805796257, 'max_depth': 8, 'subsample': 0.7687382562529482, 'colsample_bytree': 0.8292761295104989, 'gamma': 4.20837835336914, 'reg_alpha': 0.011998095235026165, 'reg_lambda': 3.7964184083355055e-07}. Best is trial 11 with value: -4.802155876159668.\n",
      "[I 2025-04-06 15:10:16,157] Trial 17 finished with value: -4.805411338806152 and parameters: {'n_estimators': 600, 'learning_rate': 0.029587501889647413, 'max_depth': 8, 'subsample': 0.6344761319100422, 'colsample_bytree': 0.8361066626568736, 'gamma': 3.248825319101993, 'reg_alpha': 0.017720074562205475, 'reg_lambda': 2.3697789755590673e-07}. Best is trial 11 with value: -4.802155876159668.\n",
      "[I 2025-04-06 15:10:30,231] Trial 18 finished with value: -4.818362140655518 and parameters: {'n_estimators': 500, 'learning_rate': 0.06424416945755392, 'max_depth': 5, 'subsample': 0.6125529232396387, 'colsample_bytree': 0.8466968916307895, 'gamma': 3.0397459630015455, 'reg_alpha': 7.196239095699418e-06, 'reg_lambda': 1.7353743400258518e-07}. Best is trial 11 with value: -4.802155876159668.\n",
      "[I 2025-04-06 15:10:42,059] Trial 19 finished with value: -4.833411312103271 and parameters: {'n_estimators': 300, 'learning_rate': 0.03249170400218046, 'max_depth': 7, 'subsample': 0.5042389049173597, 'colsample_bytree': 0.9762433059119286, 'gamma': 1.9152326576976466, 'reg_alpha': 0.018838144226282695, 'reg_lambda': 0.00015200102791434943}. Best is trial 11 with value: -4.802155876159668.\n",
      "[I 2025-04-06 15:10:48,378] Trial 20 finished with value: -4.870950222015381 and parameters: {'n_estimators': 200, 'learning_rate': 0.2489414955930654, 'max_depth': 5, 'subsample': 0.6277880780394266, 'colsample_bytree': 0.7670261279631, 'gamma': 3.495546780343079, 'reg_alpha': 3.2066745982767466e-07, 'reg_lambda': 1.8915570984744785e-06}. Best is trial 11 with value: -4.802155876159668.\n",
      "[I 2025-04-06 15:11:08,541] Trial 21 finished with value: -4.8039604187011715 and parameters: {'n_estimators': 600, 'learning_rate': 0.027491689892064484, 'max_depth': 8, 'subsample': 0.7305830691445477, 'colsample_bytree': 0.8413758216620226, 'gamma': 4.039699307698742, 'reg_alpha': 0.018399858766644083, 'reg_lambda': 1.7801239863362734e-07}. Best is trial 11 with value: -4.802155876159668.\n",
      "[I 2025-04-06 15:11:31,307] Trial 22 finished with value: -4.806127262115479 and parameters: {'n_estimators': 700, 'learning_rate': 0.029197007526075645, 'max_depth': 8, 'subsample': 0.7291948743872424, 'colsample_bytree': 0.8719566546445009, 'gamma': 2.7370420166488256, 'reg_alpha': 0.059049831019609575, 'reg_lambda': 6.129594751144607e-08}. Best is trial 11 with value: -4.802155876159668.\n",
      "[I 2025-04-06 15:11:48,968] Trial 23 finished with value: -4.8176651954650875 and parameters: {'n_estimators': 500, 'learning_rate': 0.04142551140824577, 'max_depth': 9, 'subsample': 0.6781975106654231, 'colsample_bytree': 0.7963907228117761, 'gamma': 4.149677298908226, 'reg_alpha': 0.00701526384402564, 'reg_lambda': 1.0284622433885706e-08}. Best is trial 11 with value: -4.802155876159668.\n",
      "[I 2025-04-06 15:12:12,891] Trial 24 finished with value: -4.809745407104492 and parameters: {'n_estimators': 700, 'learning_rate': 0.02328099562668424, 'max_depth': 8, 'subsample': 0.6386775559771668, 'colsample_bytree': 0.9897177999793154, 'gamma': 3.6855706322706085, 'reg_alpha': 0.00021529936965880442, 'reg_lambda': 1.284128066170469e-07}. Best is trial 11 with value: -4.802155876159668.\n",
      "[I 2025-04-06 15:12:25,163] Trial 25 finished with value: -4.813599681854248 and parameters: {'n_estimators': 400, 'learning_rate': 0.06937689212001999, 'max_depth': 7, 'subsample': 0.7297917785067971, 'colsample_bytree': 0.7024647038551364, 'gamma': 2.1758032034359567, 'reg_alpha': 0.13825804072641976, 'reg_lambda': 1.965513627516965e-06}. Best is trial 11 with value: -4.802155876159668.\n",
      "[I 2025-04-06 15:12:42,565] Trial 26 finished with value: -4.817316055297852 and parameters: {'n_estimators': 600, 'learning_rate': 0.046511824886694204, 'max_depth': 6, 'subsample': 0.5934485102919047, 'colsample_bytree': 0.8742799231206206, 'gamma': 3.2249584097949477, 'reg_alpha': 2.2205475721128084e-05, 'reg_lambda': 2.6412426002450357e-05}. Best is trial 11 with value: -4.802155876159668.\n",
      "[I 2025-04-06 15:13:04,596] Trial 27 finished with value: -4.819608974456787 and parameters: {'n_estimators': 500, 'learning_rate': 0.021428732365188514, 'max_depth': 10, 'subsample': 0.5633382589784034, 'colsample_bytree': 0.7204365869111349, 'gamma': 2.671139402231005, 'reg_alpha': 0.004852427129743432, 'reg_lambda': 4.1594357038272367e-07}. Best is trial 11 with value: -4.802155876159668.\n",
      "[I 2025-04-06 15:13:25,680] Trial 28 finished with value: -4.816858863830566 and parameters: {'n_estimators': 600, 'learning_rate': 0.03188516956808004, 'max_depth': 9, 'subsample': 0.5522627392007942, 'colsample_bytree': 0.7840652127839154, 'gamma': 4.399789610364531, 'reg_alpha': 0.8898379014412569, 'reg_lambda': 3.751198668496647e-08}. Best is trial 11 with value: -4.802155876159668.\n",
      "[I 2025-04-06 15:13:36,170] Trial 29 finished with value: -4.854270172119141 and parameters: {'n_estimators': 400, 'learning_rate': 0.18722164768293892, 'max_depth': 4, 'subsample': 0.6996494631374627, 'colsample_bytree': 0.5491985829937961, 'gamma': 1.5854752599990616, 'reg_alpha': 0.0009006906458155078, 'reg_lambda': 0.003473524309762236}. Best is trial 11 with value: -4.802155876159668.\n",
      "[I 2025-04-06 15:13:48,906] Trial 30 finished with value: -4.941046524047851 and parameters: {'n_estimators': 400, 'learning_rate': 0.16559114939647343, 'max_depth': 8, 'subsample': 0.6532406822530441, 'colsample_bytree': 0.8475839494736616, 'gamma': 0.6106173698963779, 'reg_alpha': 0.029153615648113675, 'reg_lambda': 2.1860813871294495e-07}. Best is trial 11 with value: -4.802155876159668.\n",
      "[I 2025-04-06 15:14:11,596] Trial 31 finished with value: -4.8063701629638675 and parameters: {'n_estimators': 600, 'learning_rate': 0.016269663570712738, 'max_depth': 8, 'subsample': 0.7969062058912271, 'colsample_bytree': 0.8216788675052351, 'gamma': 4.073439054085959, 'reg_alpha': 0.0072731666782437235, 'reg_lambda': 7.918691885965633e-07}. Best is trial 11 with value: -4.802155876159668.\n",
      "[I 2025-04-06 15:14:36,460] Trial 32 finished with value: -4.811453723907471 and parameters: {'n_estimators': 700, 'learning_rate': 0.01360283695813919, 'max_depth': 7, 'subsample': 0.7786851261536387, 'colsample_bytree': 0.7428861247518637, 'gamma': 3.832559993203172, 'reg_alpha': 0.20590352586545313, 'reg_lambda': 2.581694097891236e-06}. Best is trial 11 with value: -4.802155876159668.\n",
      "[I 2025-04-06 15:14:58,643] Trial 33 finished with value: -4.803448677062988 and parameters: {'n_estimators': 600, 'learning_rate': 0.01859920664566379, 'max_depth': 8, 'subsample': 0.8093210683072227, 'colsample_bytree': 0.8338999353518832, 'gamma': 4.5860264758761025, 'reg_alpha': 0.015005204115395335, 'reg_lambda': 3.062409826228339e-07}. Best is trial 11 with value: -4.802155876159668.\n",
      "[I 2025-04-06 15:15:17,926] Trial 34 finished with value: -4.808317565917969 and parameters: {'n_estimators': 500, 'learning_rate': 0.025787801858835842, 'max_depth': 9, 'subsample': 0.8858359921827591, 'colsample_bytree': 0.884572143945231, 'gamma': 4.626909794352127, 'reg_alpha': 0.002498736622834581, 'reg_lambda': 6.614205056341956e-08}. Best is trial 11 with value: -4.802155876159668.\n",
      "[I 2025-04-06 15:15:38,222] Trial 35 finished with value: -4.802669620513916 and parameters: {'n_estimators': 700, 'learning_rate': 0.037247074250556164, 'max_depth': 7, 'subsample': 0.8186885561775367, 'colsample_bytree': 0.7678833709187051, 'gamma': 4.514443017704552, 'reg_alpha': 0.03879942729968801, 'reg_lambda': 4.73173893015586e-06}. Best is trial 11 with value: -4.802155876159668.\n",
      "[I 2025-04-06 15:16:02,663] Trial 36 finished with value: -4.8123064041137695 and parameters: {'n_estimators': 900, 'learning_rate': 0.05182590950347799, 'max_depth': 7, 'subsample': 0.820449447807829, 'colsample_bytree': 0.6870710400699961, 'gamma': 4.644796314299159, 'reg_alpha': 0.05728380257868679, 'reg_lambda': 0.0006214472259011065}. Best is trial 11 with value: -4.802155876159668.\n",
      "[I 2025-04-06 15:16:21,205] Trial 37 finished with value: -4.820080852508545 and parameters: {'n_estimators': 700, 'learning_rate': 0.08501990729429237, 'max_depth': 6, 'subsample': 0.7554860479755379, 'colsample_bytree': 0.7604819540267103, 'gamma': 4.899886047546951, 'reg_alpha': 0.00030238899424246554, 'reg_lambda': 1.2437186295338171e-05}. Best is trial 11 with value: -4.802155876159668.\n",
      "[I 2025-04-06 15:16:46,465] Trial 38 finished with value: -4.802859497070313 and parameters: {'n_estimators': 900, 'learning_rate': 0.03775507590376685, 'max_depth': 7, 'subsample': 0.8208145022855263, 'colsample_bytree': 0.7395583788643971, 'gamma': 3.8511972006049136, 'reg_alpha': 0.468784001179606, 'reg_lambda': 3.96130898488517e-06}. Best is trial 11 with value: -4.802155876159668.\n",
      "[I 2025-04-06 15:17:10,612] Trial 39 finished with value: -4.800234603881836 and parameters: {'n_estimators': 900, 'learning_rate': 0.037513536394636955, 'max_depth': 6, 'subsample': 0.933647451477991, 'colsample_bytree': 0.6144275163569041, 'gamma': 4.463084680545497, 'reg_alpha': 0.8916002739319406, 'reg_lambda': 3.4534209723815736e-06}. Best is trial 39 with value: -4.800234603881836.\n",
      "[I 2025-04-06 15:17:34,367] Trial 40 finished with value: -4.800268268585205 and parameters: {'n_estimators': 900, 'learning_rate': 0.037886140367722435, 'max_depth': 6, 'subsample': 0.9631460961266339, 'colsample_bytree': 0.627225286017281, 'gamma': 4.975847614265044, 'reg_alpha': 0.5820593083210277, 'reg_lambda': 5.06292419700608e-06}. Best is trial 39 with value: -4.800234603881836.\n",
      "[I 2025-04-06 15:17:58,041] Trial 41 finished with value: -4.799409198760986 and parameters: {'n_estimators': 900, 'learning_rate': 0.038633681436004576, 'max_depth': 6, 'subsample': 0.960801107370461, 'colsample_bytree': 0.6263699801226762, 'gamma': 4.926498329907147, 'reg_alpha': 0.5687887554840704, 'reg_lambda': 5.810428411766778e-05}. Best is trial 41 with value: -4.799409198760986.\n",
      "[I 2025-04-06 15:18:23,292] Trial 42 finished with value: -4.813139343261719 and parameters: {'n_estimators': 1000, 'learning_rate': 0.06134697122233476, 'max_depth': 6, 'subsample': 0.9751324680037777, 'colsample_bytree': 0.6155162347801817, 'gamma': 4.98167924862778, 'reg_alpha': 0.8539485537603505, 'reg_lambda': 7.079907511529759e-05}. Best is trial 41 with value: -4.799409198760986.\n",
      "[I 2025-04-06 15:18:46,223] Trial 43 finished with value: -4.811110877990723 and parameters: {'n_estimators': 900, 'learning_rate': 0.03581968416428364, 'max_depth': 5, 'subsample': 0.9422185306267591, 'colsample_bytree': 0.563584787765356, 'gamma': 4.413382132398611, 'reg_alpha': 0.13465330892283991, 'reg_lambda': 7.773044165093218e-06}. Best is trial 41 with value: -4.799409198760986.\n",
      "[I 2025-04-06 15:19:07,148] Trial 44 finished with value: -4.804773616790771 and parameters: {'n_estimators': 800, 'learning_rate': 0.050024306473912795, 'max_depth': 6, 'subsample': 0.9078007331157555, 'colsample_bytree': 0.6202379206681599, 'gamma': 4.367667207402367, 'reg_alpha': 0.33766457983553094, 'reg_lambda': 0.00032463203614836023}. Best is trial 41 with value: -4.799409198760986.\n",
      "[I 2025-04-06 15:19:31,365] Trial 45 finished with value: -4.817879486083984 and parameters: {'n_estimators': 1000, 'learning_rate': 0.07471961449485655, 'max_depth': 5, 'subsample': 0.962134802481226, 'colsample_bytree': 0.5880296988475144, 'gamma': 4.956737201849851, 'reg_alpha': 0.08833486193504234, 'reg_lambda': 3.592273365337934e-05}. Best is trial 41 with value: -4.799409198760986.\n",
      "[I 2025-04-06 15:19:54,374] Trial 46 finished with value: -4.809204769134522 and parameters: {'n_estimators': 900, 'learning_rate': 0.04033616930271242, 'max_depth': 5, 'subsample': 0.9227275620500416, 'colsample_bytree': 0.6705511470943242, 'gamma': 4.788472996608949, 'reg_alpha': 0.40014379065147504, 'reg_lambda': 0.0030623670225431283}. Best is trial 41 with value: -4.799409198760986.\n",
      "[I 2025-04-06 15:20:13,973] Trial 47 finished with value: -4.820269012451172 and parameters: {'n_estimators': 800, 'learning_rate': 0.05746831134391528, 'max_depth': 4, 'subsample': 0.8732198692449973, 'colsample_bytree': 0.640319707044427, 'gamma': 1.182748751321249, 'reg_alpha': 0.1830992404229701, 'reg_lambda': 1.905602634733948e-05}. Best is trial 41 with value: -4.799409198760986.\n",
      "[I 2025-04-06 15:20:39,819] Trial 48 finished with value: -4.801820087432861 and parameters: {'n_estimators': 1000, 'learning_rate': 0.04785678255222019, 'max_depth': 6, 'subsample': 0.9913399091595717, 'colsample_bytree': 0.5350330642844725, 'gamma': 4.493501816226405, 'reg_alpha': 0.039143805603170226, 'reg_lambda': 6.960309457393213e-05}. Best is trial 41 with value: -4.799409198760986.\n",
      "[I 2025-04-06 15:21:05,426] Trial 49 finished with value: -4.799811840057373 and parameters: {'n_estimators': 1000, 'learning_rate': 0.04578326925290185, 'max_depth': 6, 'subsample': 0.9984636655265211, 'colsample_bytree': 0.5348184920156754, 'gamma': 1.8314306167777699, 'reg_alpha': 0.9557220368450722, 'reg_lambda': 7.675716756381936e-05}. Best is trial 41 with value: -4.799409198760986.\n",
      "[I 2025-04-06 15:21:05,428] A new study created in memory with name: no-name-02d9c3d3-963d-4ff6-8a6f-98b432c963f0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optuna study for XGBoost Regression complete.\n",
      "Best parameters (XGBoost Regression):  {'n_estimators': 900, 'learning_rate': 0.038633681436004576, 'max_depth': 6, 'subsample': 0.960801107370461, 'colsample_bytree': 0.6263699801226762, 'gamma': 4.926498329907147, 'reg_alpha': 0.5687887554840704, 'reg_lambda': 5.810428411766778e-05}\n",
      "Best value (Neg MAE):  -4.799409198760986\n",
      "\n",
      "Calculated scale_pos_weight for XGBClassifier Tuning: 6.42\n",
      "\n",
      "--- Running XGBoost Tuning for Classification ---\n",
      "Starting Optuna study for XGBoost Classification (F1 Score, 50 trials, 5-fold CV)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-06 15:21:12,267] Trial 0 finished with value: 0.5214314053991451 and parameters: {'n_estimators': 200, 'learning_rate': 0.08645717198597459, 'max_depth': 4, 'subsample': 0.7972351582165182, 'colsample_bytree': 0.8954887644347789, 'gamma': 3.283383429573518, 'reg_alpha': 0.0012075953732307383, 'reg_lambda': 1.06799011583065e-08}. Best is trial 0 with value: 0.5214314053991451.\n",
      "[I 2025-04-06 15:21:29,246] Trial 1 finished with value: 0.5221784532327052 and parameters: {'n_estimators': 700, 'learning_rate': 0.1665745342346827, 'max_depth': 3, 'subsample': 0.8378550466365733, 'colsample_bytree': 0.5468272003669729, 'gamma': 1.8920384463035667, 'reg_alpha': 1.1143732254766286e-08, 'reg_lambda': 0.00429446908032998}. Best is trial 1 with value: 0.5221784532327052.\n",
      "[I 2025-04-06 15:21:36,048] Trial 2 finished with value: 0.5245703666148304 and parameters: {'n_estimators': 100, 'learning_rate': 0.02723146334007002, 'max_depth': 7, 'subsample': 0.8175712139757143, 'colsample_bytree': 0.7267021300219021, 'gamma': 4.552068019192413, 'reg_alpha': 8.096828511199155e-07, 'reg_lambda': 0.008753497468200611}. Best is trial 2 with value: 0.5245703666148304.\n",
      "[I 2025-04-06 15:22:03,291] Trial 3 finished with value: 0.5034639958459192 and parameters: {'n_estimators': 900, 'learning_rate': 0.11405688067667279, 'max_depth': 8, 'subsample': 0.8517012567740734, 'colsample_bytree': 0.9386958497045408, 'gamma': 0.8369353332823726, 'reg_alpha': 1.4067331287314146e-06, 'reg_lambda': 0.17643371637139765}. Best is trial 2 with value: 0.5245703666148304.\n",
      "[I 2025-04-06 15:22:33,168] Trial 4 finished with value: 0.5103696005368635 and parameters: {'n_estimators': 1000, 'learning_rate': 0.09161255602301673, 'max_depth': 7, 'subsample': 0.7281163339576888, 'colsample_bytree': 0.9110497121167622, 'gamma': 1.5939083398691194, 'reg_alpha': 0.1877430543766404, 'reg_lambda': 0.009376283986821044}. Best is trial 2 with value: 0.5245703666148304.\n",
      "[I 2025-04-06 15:22:41,573] Trial 5 finished with value: 0.5290113982250793 and parameters: {'n_estimators': 200, 'learning_rate': 0.08204764229553374, 'max_depth': 8, 'subsample': 0.6547681129336702, 'colsample_bytree': 0.5037221138868602, 'gamma': 4.437603349106966, 'reg_alpha': 0.009150269407690873, 'reg_lambda': 1.1161332892849854e-05}. Best is trial 5 with value: 0.5290113982250793.\n",
      "[I 2025-04-06 15:22:58,912] Trial 6 finished with value: 0.5248454550699927 and parameters: {'n_estimators': 400, 'learning_rate': 0.013707284302546764, 'max_depth': 6, 'subsample': 0.5060815700746448, 'colsample_bytree': 0.6257242974230102, 'gamma': 1.7501431967234988, 'reg_alpha': 0.0003877386139753585, 'reg_lambda': 1.2716430312407196e-07}. Best is trial 5 with value: 0.5290113982250793.\n",
      "[I 2025-04-06 15:23:15,176] Trial 7 finished with value: 0.5294144074126919 and parameters: {'n_estimators': 500, 'learning_rate': 0.07079876784772111, 'max_depth': 10, 'subsample': 0.6212979769625675, 'colsample_bytree': 0.6078153167067852, 'gamma': 4.781402572838203, 'reg_alpha': 1.0301337750940453e-06, 'reg_lambda': 0.03831594694763072}. Best is trial 7 with value: 0.5294144074126919.\n",
      "[I 2025-04-06 15:23:40,234] Trial 8 finished with value: 0.5359515447155501 and parameters: {'n_estimators': 500, 'learning_rate': 0.011518311087697452, 'max_depth': 9, 'subsample': 0.8329152023884776, 'colsample_bytree': 0.5930825417879231, 'gamma': 4.855568356798562, 'reg_alpha': 0.0013354667618070827, 'reg_lambda': 2.890280601115698e-06}. Best is trial 8 with value: 0.5359515447155501.\n",
      "[I 2025-04-06 15:24:01,997] Trial 9 finished with value: 0.5114453718621578 and parameters: {'n_estimators': 900, 'learning_rate': 0.222187320834275, 'max_depth': 3, 'subsample': 0.5897806210994438, 'colsample_bytree': 0.6689862730844217, 'gamma': 3.683629860158475, 'reg_alpha': 0.014768516575516966, 'reg_lambda': 1.756045597091506e-07}. Best is trial 8 with value: 0.5359515447155501.\n",
      "[I 2025-04-06 15:24:35,912] Trial 10 finished with value: 0.535193876155699 and parameters: {'n_estimators': 700, 'learning_rate': 0.011465516156211076, 'max_depth': 10, 'subsample': 0.9530790577034327, 'colsample_bytree': 0.8075374242400555, 'gamma': 2.9848808273546785, 'reg_alpha': 1.839891805824368e-05, 'reg_lambda': 3.7949787824925785e-05}. Best is trial 8 with value: 0.5359515447155501.\n",
      "[I 2025-04-06 15:25:12,180] Trial 11 finished with value: 0.5337659616968885 and parameters: {'n_estimators': 700, 'learning_rate': 0.010398478779492607, 'max_depth': 10, 'subsample': 0.9792879810051726, 'colsample_bytree': 0.82463298534901, 'gamma': 3.112423650766017, 'reg_alpha': 3.227521389162081e-05, 'reg_lambda': 4.2733389018924576e-05}. Best is trial 8 with value: 0.5359515447155501.\n",
      "[I 2025-04-06 15:25:37,405] Trial 12 finished with value: 0.5299049283423978 and parameters: {'n_estimators': 700, 'learning_rate': 0.028262160735972597, 'max_depth': 9, 'subsample': 0.9581072076260735, 'colsample_bytree': 0.8040847666428065, 'gamma': 0.07830632733465093, 'reg_alpha': 2.0583602592485285e-05, 'reg_lambda': 3.250346277434901e-06}. Best is trial 8 with value: 0.5359515447155501.\n",
      "[I 2025-04-06 15:25:56,479] Trial 13 finished with value: 0.5344517513904901 and parameters: {'n_estimators': 400, 'learning_rate': 0.020297781310375518, 'max_depth': 9, 'subsample': 0.9087598158126222, 'colsample_bytree': 0.7369410425405307, 'gamma': 2.7502610170561335, 'reg_alpha': 0.24607221720372555, 'reg_lambda': 0.0005209617021911512}. Best is trial 8 with value: 0.5359515447155501.\n",
      "[I 2025-04-06 15:26:15,072] Trial 14 finished with value: 0.5293674455845239 and parameters: {'n_estimators': 600, 'learning_rate': 0.04078615287513151, 'max_depth': 10, 'subsample': 0.9062325084876582, 'colsample_bytree': 0.8232291108729342, 'gamma': 3.975012019881168, 'reg_alpha': 8.745148683305935e-05, 'reg_lambda': 0.00017247454405132468}. Best is trial 8 with value: 0.5359515447155501.\n",
      "[I 2025-04-06 15:26:32,763] Trial 15 finished with value: 0.5197736538810956 and parameters: {'n_estimators': 500, 'learning_rate': 0.015780567815425275, 'max_depth': 5, 'subsample': 0.9069964204201606, 'colsample_bytree': 0.6549347877619242, 'gamma': 2.3258842077441564, 'reg_alpha': 0.004970256842264873, 'reg_lambda': 1.3831309157309233e-06}. Best is trial 8 with value: 0.5359515447155501.\n",
      "[I 2025-04-06 15:27:08,330] Trial 16 finished with value: 0.5293950484810639 and parameters: {'n_estimators': 800, 'learning_rate': 0.010398849618679658, 'max_depth': 9, 'subsample': 0.7495224908490234, 'colsample_bytree': 0.9883990806075174, 'gamma': 4.060309751168584, 'reg_alpha': 5.700140245243363e-06, 'reg_lambda': 0.000777607849700387}. Best is trial 8 with value: 0.5359515447155501.\n",
      "[I 2025-04-06 15:27:23,437] Trial 17 finished with value: 0.5356418079337935 and parameters: {'n_estimators': 400, 'learning_rate': 0.0444931170422212, 'max_depth': 8, 'subsample': 0.6995516607512848, 'colsample_bytree': 0.5730163179200475, 'gamma': 3.421121554226669, 'reg_alpha': 7.656101465740643e-08, 'reg_lambda': 3.772650361503545e-05}. Best is trial 8 with value: 0.5359515447155501.\n",
      "[I 2025-04-06 15:27:35,285] Trial 18 finished with value: 0.5326383386928251 and parameters: {'n_estimators': 300, 'learning_rate': 0.04567407533665088, 'max_depth': 8, 'subsample': 0.6728934380554011, 'colsample_bytree': 0.5628176677165406, 'gamma': 4.917787129112819, 'reg_alpha': 1.1676853536144614e-08, 'reg_lambda': 6.516176761652769e-07}. Best is trial 8 with value: 0.5359515447155501.\n",
      "[I 2025-04-06 15:27:49,552] Trial 19 finished with value: 0.5292744927359962 and parameters: {'n_estimators': 400, 'learning_rate': 0.03206924294976073, 'max_depth': 6, 'subsample': 0.6893642393632452, 'colsample_bytree': 0.5783357343191193, 'gamma': 3.632544281303497, 'reg_alpha': 1.0395354129137842e-07, 'reg_lambda': 7.855809610053032e-06}. Best is trial 8 with value: 0.5359515447155501.\n",
      "[I 2025-04-06 15:28:03,544] Trial 20 finished with value: 0.5263698794892043 and parameters: {'n_estimators': 300, 'learning_rate': 0.01990748943676069, 'max_depth': 7, 'subsample': 0.7813032686597943, 'colsample_bytree': 0.5173235241060227, 'gamma': 2.395645163474309, 'reg_alpha': 0.0003061069397500011, 'reg_lambda': 1.6791735488117854e-08}. Best is trial 8 with value: 0.5359515447155501.\n",
      "[I 2025-04-06 15:28:30,701] Trial 21 finished with value: 0.5305262348287527 and parameters: {'n_estimators': 600, 'learning_rate': 0.012808195094184108, 'max_depth': 9, 'subsample': 0.8679225060867609, 'colsample_bytree': 0.7133135124038652, 'gamma': 2.9012477235193574, 'reg_alpha': 8.869005293789691e-08, 'reg_lambda': 3.949775638246777e-05}. Best is trial 8 with value: 0.5359515447155501.\n",
      "[I 2025-04-06 15:28:51,123] Trial 22 finished with value: 0.5284501908917896 and parameters: {'n_estimators': 500, 'learning_rate': 0.019237712027705193, 'max_depth': 8, 'subsample': 0.9408684665747336, 'colsample_bytree': 0.7878810335691808, 'gamma': 3.4319873499890106, 'reg_alpha': 8.818202016026795e-06, 'reg_lambda': 0.00013201674911530732}. Best is trial 8 with value: 0.5359515447155501.\n",
      "[I 2025-04-06 15:29:09,800] Trial 23 finished with value: 0.5293877391409472 and parameters: {'n_estimators': 600, 'learning_rate': 0.05922948916648521, 'max_depth': 10, 'subsample': 0.7128981391939518, 'colsample_bytree': 0.675705109201815, 'gamma': 4.230740849557332, 'reg_alpha': 0.052253469017163254, 'reg_lambda': 1.6775057161931795e-05}. Best is trial 8 with value: 0.5359515447155501.\n",
      "[I 2025-04-06 15:29:42,375] Trial 24 finished with value: 0.5331221665697623 and parameters: {'n_estimators': 800, 'learning_rate': 0.016248587245501902, 'max_depth': 9, 'subsample': 0.7736013878979362, 'colsample_bytree': 0.8627337918360489, 'gamma': 1.1444097819039531, 'reg_alpha': 0.0011992538317431164, 'reg_lambda': 0.0007568136002417188}. Best is trial 8 with value: 0.5359515447155501.\n",
      "[I 2025-04-06 15:29:56,814] Trial 25 finished with value: 0.5311909825486788 and parameters: {'n_estimators': 300, 'learning_rate': 0.022756626201786787, 'max_depth': 8, 'subsample': 0.9869428906632003, 'colsample_bytree': 0.6195390555044906, 'gamma': 2.712073800281945, 'reg_alpha': 1.4289358616825718e-07, 'reg_lambda': 3.069227719733338e-07}. Best is trial 8 with value: 0.5359515447155501.\n",
      "[I 2025-04-06 15:30:13,823] Trial 26 finished with value: 0.5289179165770858 and parameters: {'n_estimators': 400, 'learning_rate': 0.03634148410603824, 'max_depth': 10, 'subsample': 0.5528028258929554, 'colsample_bytree': 0.5910487100362775, 'gamma': 3.8073252817800594, 'reg_alpha': 0.00020981532809827386, 'reg_lambda': 2.662768197532148e-06}. Best is trial 8 with value: 0.5359515447155501.\n",
      "[I 2025-04-06 15:30:28,320] Trial 27 finished with value: 0.5234979681777745 and parameters: {'n_estimators': 500, 'learning_rate': 0.1486984515527222, 'max_depth': 9, 'subsample': 0.8792474957203018, 'colsample_bytree': 0.7800154349582953, 'gamma': 2.109509131427549, 'reg_alpha': 0.0018180433009407234, 'reg_lambda': 0.7744313587086902}. Best is trial 8 with value: 0.5359515447155501.\n",
      "[I 2025-04-06 15:30:47,247] Trial 28 finished with value: 0.5099150559313139 and parameters: {'n_estimators': 800, 'learning_rate': 0.28551381272489346, 'max_depth': 7, 'subsample': 0.8180958128020033, 'colsample_bytree': 0.5434341612604088, 'gamma': 3.1704793666374194, 'reg_alpha': 7.26580163514621e-05, 'reg_lambda': 5.464554149239361e-05}. Best is trial 8 with value: 0.5359515447155501.\n",
      "[I 2025-04-06 15:31:01,926] Trial 29 finished with value: 0.5344164809205053 and parameters: {'n_estimators': 200, 'learning_rate': 0.012431758845174033, 'max_depth': 10, 'subsample': 0.7568045525520279, 'colsample_bytree': 0.6895482382391611, 'gamma': 3.465970678309138, 'reg_alpha': 4.306939767813636e-06, 'reg_lambda': 7.572790396312873e-08}. Best is trial 8 with value: 0.5359515447155501.\n",
      "[I 2025-04-06 15:31:18,030] Trial 30 finished with value: 0.5282779244010529 and parameters: {'n_estimators': 600, 'learning_rate': 0.051741890037747734, 'max_depth': 5, 'subsample': 0.9340217360592049, 'colsample_bytree': 0.6417310418406319, 'gamma': 4.368585699180684, 'reg_alpha': 2.66082922605846e-07, 'reg_lambda': 0.00026635803035161143}. Best is trial 8 with value: 0.5359515447155501.\n",
      "[I 2025-04-06 15:31:38,330] Trial 31 finished with value: 0.5295217260212765 and parameters: {'n_estimators': 400, 'learning_rate': 0.017498405303944493, 'max_depth': 9, 'subsample': 0.8903047636512296, 'colsample_bytree': 0.7540894058792378, 'gamma': 2.7281990989187905, 'reg_alpha': 0.7255109035357515, 'reg_lambda': 0.000739840525850729}. Best is trial 8 with value: 0.5359515447155501.\n",
      "[I 2025-04-06 15:31:53,799] Trial 32 finished with value: 0.5308622861067732 and parameters: {'n_estimators': 300, 'learning_rate': 0.022604604430633234, 'max_depth': 9, 'subsample': 0.8088302982377634, 'colsample_bytree': 0.8553357330647393, 'gamma': 2.9470982579593716, 'reg_alpha': 0.06324248756050523, 'reg_lambda': 0.0016253353011987245}. Best is trial 8 with value: 0.5359515447155501.\n",
      "[I 2025-04-06 15:32:01,425] Trial 33 finished with value: 0.5241895099754131 and parameters: {'n_estimators': 100, 'learning_rate': 0.013970184914320342, 'max_depth': 8, 'subsample': 0.9245518492122882, 'colsample_bytree': 0.7205731192425191, 'gamma': 2.5959687824862776, 'reg_alpha': 0.6229034719018202, 'reg_lambda': 1.5846097880075373e-05}. Best is trial 8 with value: 0.5359515447155501.\n",
      "[I 2025-04-06 15:32:33,837] Trial 34 finished with value: 0.531874302408153 and parameters: {'n_estimators': 700, 'learning_rate': 0.01096940134621592, 'max_depth': 9, 'subsample': 0.8577928765276164, 'colsample_bytree': 0.7600689067990953, 'gamma': 2.1107424025654833, 'reg_alpha': 2.217571031231775e-08, 'reg_lambda': 0.0038100510896375834}. Best is trial 8 with value: 0.5359515447155501.\n",
      "[I 2025-04-06 15:32:52,917] Trial 35 finished with value: 0.5381897693492729 and parameters: {'n_estimators': 400, 'learning_rate': 0.02404725697718134, 'max_depth': 10, 'subsample': 0.8464161577053163, 'colsample_bytree': 0.5384129296787337, 'gamma': 1.284020793833157, 'reg_alpha': 0.0010937946130121804, 'reg_lambda': 0.00034656017495160576}. Best is trial 35 with value: 0.5381897693492729.\n",
      "[I 2025-04-06 15:33:14,449] Trial 36 finished with value: 0.5345419283228503 and parameters: {'n_estimators': 500, 'learning_rate': 0.024945102421708085, 'max_depth': 10, 'subsample': 0.8340381375503707, 'colsample_bytree': 0.559623257407106, 'gamma': 0.6701385983607849, 'reg_alpha': 0.001765609087475155, 'reg_lambda': 5.6359769424001285e-06}. Best is trial 35 with value: 0.5381897693492729.\n",
      "[I 2025-04-06 15:33:39,030] Trial 37 finished with value: 0.5304468172124134 and parameters: {'n_estimators': 600, 'learning_rate': 0.030193556802771852, 'max_depth': 10, 'subsample': 0.7375858442516127, 'colsample_bytree': 0.5364737994569123, 'gamma': 1.3879194034359663, 'reg_alpha': 0.0005831793852629956, 'reg_lambda': 7.957850064551185e-05}. Best is trial 35 with value: 0.5381897693492729.\n",
      "[I 2025-04-06 15:33:48,903] Trial 38 finished with value: 0.5315637841638117 and parameters: {'n_estimators': 200, 'learning_rate': 0.03733741944253494, 'max_depth': 8, 'subsample': 0.7914120768172915, 'colsample_bytree': 0.5141165364855348, 'gamma': 0.4054867100714832, 'reg_alpha': 0.019610270216104196, 'reg_lambda': 0.02608981324078511}. Best is trial 35 with value: 0.5381897693492729.\n",
      "[I 2025-04-06 15:34:05,088] Trial 39 finished with value: 0.5188367181846202 and parameters: {'n_estimators': 400, 'learning_rate': 0.06800593076677353, 'max_depth': 10, 'subsample': 0.7013976885982395, 'colsample_bytree': 0.5824213712446198, 'gamma': 1.2201306161298713, 'reg_alpha': 0.007052690591357083, 'reg_lambda': 1.0425580727418134e-06}. Best is trial 35 with value: 0.5381897693492729.\n",
      "[I 2025-04-06 15:34:40,532] Trial 40 finished with value: 0.5337086134556285 and parameters: {'n_estimators': 1000, 'learning_rate': 0.015283391957580626, 'max_depth': 8, 'subsample': 0.6377876382188772, 'colsample_bytree': 0.6027056703339727, 'gamma': 4.7049030061949475, 'reg_alpha': 4.313693313560711e-07, 'reg_lambda': 4.2884349773080675e-05}. Best is trial 35 with value: 0.5381897693492729.\n",
      "[I 2025-04-06 15:35:02,273] Trial 41 finished with value: 0.5339140821511394 and parameters: {'n_estimators': 500, 'learning_rate': 0.02730498871499509, 'max_depth': 10, 'subsample': 0.8297307832714398, 'colsample_bytree': 0.5537821484072603, 'gamma': 0.7796701464100753, 'reg_alpha': 0.002547241165179152, 'reg_lambda': 5.2444827076831765e-06}. Best is trial 35 with value: 0.5381897693492729.\n",
      "[I 2025-04-06 15:35:23,741] Trial 42 finished with value: 0.5342333506083228 and parameters: {'n_estimators': 500, 'learning_rate': 0.025175660378902252, 'max_depth': 10, 'subsample': 0.8322297814606765, 'colsample_bytree': 0.5253821003106953, 'gamma': 0.7359103316883027, 'reg_alpha': 2.007137315309723e-06, 'reg_lambda': 1.5329255306257006e-05}. Best is trial 35 with value: 0.5381897693492729.\n",
      "[I 2025-04-06 15:35:38,531] Trial 43 finished with value: 0.5140126136474162 and parameters: {'n_estimators': 400, 'learning_rate': 0.09382282213462141, 'max_depth': 10, 'subsample': 0.9671931172505428, 'colsample_bytree': 0.5601142655180469, 'gamma': 0.35060896802875996, 'reg_alpha': 0.0007661343361678427, 'reg_lambda': 2.248261678086158e-06}. Best is trial 35 with value: 0.5381897693492729.\n",
      "[I 2025-04-06 15:36:01,820] Trial 44 finished with value: 0.5268875482988171 and parameters: {'n_estimators': 700, 'learning_rate': 0.04781020480144315, 'max_depth': 9, 'subsample': 0.8498673867777155, 'colsample_bytree': 0.625752345729525, 'gamma': 1.5172251062849915, 'reg_alpha': 0.00013495457981491736, 'reg_lambda': 2.335799281133365e-05}. Best is trial 35 with value: 0.5381897693492729.\n",
      "[I 2025-04-06 15:36:26,999] Trial 45 finished with value: 0.5341476995062123 and parameters: {'n_estimators': 500, 'learning_rate': 0.012333172623651887, 'max_depth': 9, 'subsample': 0.7938632318242571, 'colsample_bytree': 0.569735224116109, 'gamma': 1.025900624443457, 'reg_alpha': 3.18106633560532e-05, 'reg_lambda': 5.365580962230707e-06}. Best is trial 35 with value: 0.5381897693492729.\n",
      "[I 2025-04-06 15:36:41,336] Trial 46 finished with value: 0.5356183828763978 and parameters: {'n_estimators': 300, 'learning_rate': 0.03409669284605743, 'max_depth': 10, 'subsample': 0.8883856401896864, 'colsample_bytree': 0.5047211862360207, 'gamma': 1.8019031278486897, 'reg_alpha': 0.0036548180176558436, 'reg_lambda': 0.00024716607659453367}. Best is trial 35 with value: 0.5381897693492729.\n",
      "[I 2025-04-06 15:36:54,716] Trial 47 finished with value: 0.5369608731990716 and parameters: {'n_estimators': 300, 'learning_rate': 0.03291713825163706, 'max_depth': 9, 'subsample': 0.9458475281570584, 'colsample_bytree': 0.5017413806093586, 'gamma': 2.0610198276166227, 'reg_alpha': 0.028755256281456625, 'reg_lambda': 0.00025745049675719023}. Best is trial 35 with value: 0.5381897693492729.\n",
      "[I 2025-04-06 15:37:06,946] Trial 48 finished with value: 0.5303847064530334 and parameters: {'n_estimators': 300, 'learning_rate': 0.0333632208073093, 'max_depth': 7, 'subsample': 0.8928025970926707, 'colsample_bytree': 0.5008715199676095, 'gamma': 1.7925268435852124, 'reg_alpha': 0.033113753364282544, 'reg_lambda': 0.00029596377526196725}. Best is trial 35 with value: 0.5381897693492729.\n",
      "[I 2025-04-06 15:37:13,350] Trial 49 finished with value: 0.5263195316305653 and parameters: {'n_estimators': 100, 'learning_rate': 0.04371275040145634, 'max_depth': 8, 'subsample': 0.5968358490461545, 'colsample_bytree': 0.5269580606665698, 'gamma': 2.030321329189469, 'reg_alpha': 0.0035147312052940735, 'reg_lambda': 0.0019881930392981213}. Best is trial 35 with value: 0.5381897693492729.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optuna study for XGBoost Classification complete.\n",
      "Best parameters (XGBoost Classification):  {'n_estimators': 400, 'learning_rate': 0.02404725697718134, 'max_depth': 10, 'subsample': 0.8464161577053163, 'colsample_bytree': 0.5384129296787337, 'gamma': 1.284020793833157, 'reg_alpha': 0.0010937946130121804, 'reg_lambda': 0.00034656017495160576}\n",
      "Best value (F1 Score):  0.5381897693492729\n",
      "--- STAGE 4 COMPLETE ---\n"
     ]
    }
   ],
   "source": [
    "# --- STAGE 4: Hyperparameter Tuning with Optuna ---\n",
    "\n",
    "import optuna\n",
    "import numpy as np\n",
    "import xgboost as xgb # Import XGBoost\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier # Keep RF if you want to compare later\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# Make sure necessary variables from Block 3 are available:\n",
    "# preprocessor, X_train, y_train_regr, y_train_clas\n",
    "\n",
    "print(\"--- STAGE 4 START ---\")\n",
    "\n",
    "N_TRIALS = 50 # Number of Optuna trials (increase for better search, e.g., 50-100)\n",
    "CV_FOLDS = 5 # Number of cross-validation folds\n",
    "\n",
    "# --- (Optional) Keep Original RF Tuning ---\n",
    "# You can keep or comment out the RF tuning if you want\n",
    "# print(\"\\n--- Running Random Forest Tuning (as reference) ---\")\n",
    "# ... (Objective functions and studies for RF from previous Block 4 can remain here if desired) ...\n",
    "# best_params_regr = study_regr.best_params # Store RF results if kept\n",
    "# best_params_clas = study_clas.best_params # Store RF results if kept\n",
    "\n",
    "\n",
    "# --- 4e. Objective Function for XGBoost Regression ---\n",
    "def objective_regr_xgb(trial):\n",
    "    # Suggest hyperparameters for XGBoost Regressor\n",
    "    param = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000, step=100),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0), # Fraction of samples used per tree\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0), # Fraction of features used per tree\n",
    "        'gamma': trial.suggest_float('gamma', 0, 5), # Minimum loss reduction required to make a further partition\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 1.0, log=True), # L1 regularization\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 1.0, log=True), # L2 regularization\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "\n",
    "    model = xgb.XGBRegressor(**param)\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('regressor', model)])\n",
    "    score = cross_val_score(pipeline, X_train, y_train_regr, cv=CV_FOLDS, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "    return score.mean()\n",
    "\n",
    "# --- 4f. Run Optuna Study for XGBoost Regression ---\n",
    "print(f\"\\n--- Running XGBoost Tuning for Regression ---\")\n",
    "print(f\"Starting Optuna study for XGBoost Regression (MAE, {N_TRIALS} trials, {CV_FOLDS}-fold CV)...\")\n",
    "study_regr_xgb = optuna.create_study(direction='maximize') # Maximize neg_mean_absolute_error\n",
    "study_regr_xgb.optimize(objective_regr_xgb, n_trials=N_TRIALS)\n",
    "\n",
    "print(\"Optuna study for XGBoost Regression complete.\")\n",
    "print(\"Best parameters (XGBoost Regression): \", study_regr_xgb.best_params)\n",
    "print(\"Best value (Neg MAE): \", study_regr_xgb.best_value)\n",
    "best_params_regr_xgb = study_regr_xgb.best_params\n",
    "\n",
    "\n",
    "# --- 4g. Objective Function for XGBoost Classification ---\n",
    "# Calculate scale_pos_weight (needs y_train_clas)\n",
    "neg_count = np.sum(y_train_clas == 0)\n",
    "pos_count = np.sum(y_train_clas == 1)\n",
    "scale_pos_weight_val = neg_count / pos_count if pos_count > 0 else 1\n",
    "print(f\"\\nCalculated scale_pos_weight for XGBClassifier Tuning: {scale_pos_weight_val:.2f}\")\n",
    "\n",
    "def objective_clas_xgb(trial):\n",
    "    # Suggest hyperparameters for XGBoost Classifier\n",
    "    param = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000, step=100),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 5),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 1.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 1.0, log=True),\n",
    "        'scale_pos_weight': scale_pos_weight_val, # Use pre-calculated value\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "\n",
    "    model = xgb.XGBClassifier(**param)\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', model)])\n",
    "    score = cross_val_score(pipeline, X_train, y_train_clas, cv=CV_FOLDS, scoring='f1', n_jobs=-1) # Optimize F1\n",
    "    return score.mean()\n",
    "\n",
    "# --- 4h. Run Optuna Study for XGBoost Classification ---\n",
    "print(f\"\\n--- Running XGBoost Tuning for Classification ---\")\n",
    "print(f\"Starting Optuna study for XGBoost Classification (F1 Score, {N_TRIALS} trials, {CV_FOLDS}-fold CV)...\")\n",
    "study_clas_xgb = optuna.create_study(direction='maximize') # Maximize F1 score\n",
    "study_clas_xgb.optimize(objective_clas_xgb, n_trials=N_TRIALS)\n",
    "\n",
    "print(\"Optuna study for XGBoost Classification complete.\")\n",
    "print(\"Best parameters (XGBoost Classification): \", study_clas_xgb.best_params)\n",
    "print(\"Best value (F1 Score): \", study_clas_xgb.best_value)\n",
    "best_params_clas_xgb = study_clas_xgb.best_params\n",
    "\n",
    "\n",
    "print(\"--- STAGE 4 COMPLETE ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "09b1f595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- STAGE 5 START (Using Tuned Random Forest with FULL features) ---\n",
      "Defining final pipelines using best parameters found previously by Optuna for Random Forest...\n",
      "Training final Tuned Random Forest Regression model on full training data...\n",
      "Final Tuned Random Forest Regression model trained.\n",
      "Training final Tuned Random Forest Classification model on full training data...\n",
      "Final Tuned Random Forest Classification model trained.\n",
      "\n",
      "--- FINAL Tuned Random Forest Regression Model Evaluation (on Test Set with FULL features) ---\n",
      "FINAL Mean Absolute Error (MAE) on Test Set: 4.9244\n",
      "\n",
      "--- FINAL Tuned Random Forest Classification Model Evaluation (on Test Set with FULL features) ---\n",
      "FINAL Accuracy on Test Set: 0.8300\n",
      "\n",
      "FINAL Classification Report (on Test Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   No Podium       0.96      0.84      0.90      4414\n",
      "      Podium       0.42      0.75      0.54       661\n",
      "\n",
      "    accuracy                           0.83      5075\n",
      "   macro avg       0.69      0.80      0.72      5075\n",
      "weighted avg       0.89      0.83      0.85      5075\n",
      "\n",
      "--- STAGE 5 COMPLETE (Using Tuned Random Forest with FULL features) ---\n"
     ]
    }
   ],
   "source": [
    "# --- STAGE 5: Final Model Training & Evaluation (Using Tuned Random Forest) ---\n",
    "\n",
    "# --- Imports for this block ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error, accuracy_score, classification_report\n",
    "# Import Random Forest models\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "# Make sure necessary variables are available from previous blocks:\n",
    "# preprocessor,\n",
    "# best_params_regr, best_params_clas, # <<< Use the ORIGINAL RF params from Optuna\n",
    "# X_train, y_train_regr, y_train_clas, X_test, y_test_regr, y_test_clas\n",
    "\n",
    "print(\"--- STAGE 5 START (Using Tuned Random Forest with FULL features) ---\")\n",
    "\n",
    "# --- 5a. Define Final Pipelines with Tuned Random Forest Parameters ---\n",
    "print(\"Defining final pipelines using best parameters found previously by Optuna for Random Forest...\")\n",
    "\n",
    "# Check if best parameters for RF were found/are available\n",
    "# If you didn't store them previously, you might need to re-run the RF tuning part of Block 4\n",
    "# or use the last known good parameters manually.\n",
    "if 'best_params_regr' not in locals():\n",
    "    print(\"Warning: Best RandomForest Regressor params ('best_params_regr') not found! Using defaults as fallback.\")\n",
    "    best_params_regr = {} # Fallback to defaults\n",
    "if 'best_params_clas' not in locals():\n",
    "    print(\"Warning: Best RandomForest Classifier params ('best_params_clas') not found! Using defaults as fallback.\")\n",
    "    best_params_clas = {} # Fallback to defaults\n",
    "\n",
    "# Define RandomForest Regressor using parameters found previously by Optuna\n",
    "final_rf_regressor = RandomForestRegressor(\n",
    "    **best_params_regr, # Unpack the best RF parameters\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Define RandomForest Classifier using parameters found previously by Optuna\n",
    "final_rf_classifier = RandomForestClassifier(\n",
    "    **best_params_clas, # Unpack the best RF parameters\n",
    "    random_state=42,\n",
    "    class_weight='balanced', # Ensure class_weight is retained\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Create full pipelines with TUNED Random Forest models\n",
    "final_pipeline_regr = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', final_rf_regressor) # Use Tuned RF Regressor\n",
    "])\n",
    "\n",
    "final_pipeline_clas = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', final_rf_classifier) # Use Tuned RF Classifier\n",
    "])\n",
    "\n",
    "# --- 5b. Train Final Models on FULL Training Data ---\n",
    "print(\"Training final Tuned Random Forest Regression model on full training data...\")\n",
    "final_pipeline_regr.fit(X_train, y_train_regr)\n",
    "print(\"Final Tuned Random Forest Regression model trained.\")\n",
    "\n",
    "print(\"Training final Tuned Random Forest Classification model on full training data...\")\n",
    "final_pipeline_clas.fit(X_train, y_train_clas)\n",
    "print(\"Final Tuned Random Forest Classification model trained.\")\n",
    "\n",
    "# --- 5c. Evaluate Final Models on HELD-OUT Test Data ---\n",
    "print(\"\\n--- FINAL Tuned Random Forest Regression Model Evaluation (on Test Set with FULL features) ---\")\n",
    "y_pred_regr_final = final_pipeline_regr.predict(X_test)\n",
    "mae_final = mean_absolute_error(y_test_regr, y_pred_regr_final) # RF usually predicts integers if target is int\n",
    "print(f\"FINAL Mean Absolute Error (MAE) on Test Set: {mae_final:.4f}\")\n",
    "\n",
    "print(\"\\n--- FINAL Tuned Random Forest Classification Model Evaluation (on Test Set with FULL features) ---\")\n",
    "y_pred_clas_final = final_pipeline_clas.predict(X_test)\n",
    "accuracy_final = accuracy_score(y_test_clas, y_pred_clas_final)\n",
    "print(f\"FINAL Accuracy on Test Set: {accuracy_final:.4f}\")\n",
    "print(\"\\nFINAL Classification Report (on Test Set):\")\n",
    "print(classification_report(y_test_clas, y_pred_clas_final, target_names=['No Podium', 'Podium']))\n",
    "\n",
    "print(\"--- STAGE 5 COMPLETE (Using Tuned Random Forest with FULL features) ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
